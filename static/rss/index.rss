<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>BLCV - Bartosz Ludwiczuk Computer Vision</title><description>Computer Vision Blog and Consulting</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>BLCV - Bartosz Ludwiczuk Computer Vision</title><link>http://localhost:2368/</link></image><generator>Ghost 1.15</generator><lastBuildDate>Wed, 28 Feb 2018 06:40:08 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Demystifying Face Recognition V: Data Augmentation</title><description>What are best Data-Augmentation technique which can be used for Face-Recognition?</description><link>http://localhost:2368/2018/02/27/demystifying-face-recognition-v-data-augmentation/</link><guid isPermaLink="false">5a8affd970d6352181f0afe0</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Tue, 27 Feb 2018 16:56:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/02/data_aug_erase-1.jpg" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2018/02/data_aug_erase-1.jpg" alt="Demystifying Face Recognition V: Data Augmentation"&gt;&lt;p&gt;From the beginning of creation of Neural Network, one of the biggest problem was overfitting, what means weak generalization in dataset not seen during draining (it is often named as &lt;code&gt;Generalization Gap&lt;/code&gt;). Mainly it was caused by many parameters (ex. AlexNet have 60M, VGG-Net have 230M). In Machine Learning there exist a theorem that the number of parameters should be lower than number of training example, what is normally not the case when using Deep Learning. So what are the main method which enable Deep Learning achieve superior results? By using different technique to reduce overfitting, where 3 of them have the biggest impact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weight-decay: preventing weight from achieving very high values, what can cause remembering training examples&lt;/li&gt;
&lt;li&gt;DropOut: preventing co-adaptation of features by randomly removing them&lt;/li&gt;
&lt;li&gt;Data Augmentation: technique for generating new training examples by sampling original images with modification&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to know more about first two, I advice to watch this video from &lt;a href="https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk"&gt;Stanford University&lt;/a&gt;, they should clarify you how and why they can work. In this blog-post we will focus of the third one. But firstly some of the historical information. As most of you know, the real breakthrough in Computer Vision was caused by winning entry from Geoffrey Hinton team in ImageNet, which learn Deep Convolutional Network having 60M parameters. How they manage to do so? By preventing overfitting using DropOut and Data Augmentation. Exactly, they use Random-Crop (very powerful technique  according to many papers),  flipping image Right-Left and Random Lighting Perturbation (changing the intensities of images).  So what they do exactly? In fact, they make their dataset many times bigger what cause that network does not see same image many times. Also, it was in line with beliefs Hinton about &lt;code&gt;How does the brain works?&lt;/code&gt;, as he believed that brain are making image transformation. Referring to statement &lt;code&gt;Father of Deep Learning&lt;/code&gt;, did you see fanny video which animate his story (it was presented at NIPS conference)? If not, it is worth watching:&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/mlXzufEk-2E?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;p&gt;In next year’s the more powerful networks was designed, which also need a more powerful technique for preventing overfitting. &lt;a href="https://arxiv.org/abs/1409.4842"&gt;GoogLeNet&lt;/a&gt; does exactly that by adding Random-Resized-Crop technique (it does scale image by making it smaller or bigger with changed aspect ratio followed by Random-Crop).  As you can suppose, this is not end as researcher found this as a easy and fulfilling expectations method for getting better model. A very extreme way of checking it was done by researcher from &lt;a href="https://arxiv.org/abs/1501.02876"&gt;Baidu&lt;/a&gt;, which make use from a  large number of Data-Augmentation technique achieving best results of that time. Most of their technique include extending image perturbation by changing intensities values, but they not settle down in Deep Learning community.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" src="http://localhost:2368/content/images/2018/02/baidu.png"&gt;
&lt;/p&gt;
&lt;p&gt;In summary, the following general Image Transformation was introduced:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random-Crop&lt;/li&gt;
&lt;li&gt;Random-Flipping&lt;/li&gt;
&lt;li&gt;Random-Resized-Crop&lt;/li&gt;
&lt;li&gt;Random-Perspective-Transformation&lt;/li&gt;
&lt;li&gt;Random-Image-Lighting-Perturbation&lt;/li&gt;
&lt;li&gt;Random-Image-Color-Perturbation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some papers/blogs which benchmark this technique on general Image problem, like&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1708.06020v1"&gt;Improving Deep Learning using Generic Data Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gombru.github.io/2017/09/14/data_augmentation/"&gt;Data augmentation benchmarking on CNN training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We said, that there are so many advantages of Data Augmentation technique. Are there any &lt;code&gt;Dark Side&lt;/code&gt; of them? It’s said that there are two of them. First disadvantage is possibility of model underfitting to data because of too heavy regularization method (ex. When we use weight-decay, DropOut and several DA technique). Then  we have two options: making regularization less aggravating or trying the bigger model. At a research stage we recommend taking the first option and then gradually adding another regularization method. The second disadvantage is difference between distribution of training set and test set. Using Data-Augmentation make images look much different from their original counterpart then our network may not work nice at test set, so generalization gap may be bigger than we except. For such situation many researcher propose to learn the network without any DA technique for couple of epoch after out network does not improve. We think that there could be the third disadvantage also, we will explain it later.&lt;/p&gt;
&lt;p&gt;But what about the Face Recognition? Here story is different as the aim is different. First of all, in contrast to general image problem, object (exactly face) which need to be recognized is already detected. Also, most of time the face is aligned to have always the same position to make learning easier. This is why most of the time Face-Recognition are using just Random-Horizontal-Flipping (as it does not change the position of face). Rarely Random-Crop is used, but it look like it also may work. There is one nice paper which already benchmarked many available technique, I advice to go through it, &lt;a href="https://www.sciencedirect.com/science/article/pii/S0925231216315016"&gt;Data Augmentation for Face Recognition&lt;/a&gt;. Also there are several new Data Augmentation technique which are not widely tested, maybe they will we working for our case? So, our questions which we will asking and trying to answer will be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;What are best Data-Augmentation technique which can be used for Face-Recognition?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What combination of Technique Works best?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Can we beat model with DropOut&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;h2 id="baseexperiments"&gt;Base Experiments&lt;/h2&gt;
&lt;p&gt;So what exactly technique we will be using? Here is the image with all base technique which we will be using. Most of them are basic one, but we also include many technique from fantastic Image-Augmentation library &lt;a href="http://imgaug.readthedocs.io/en/latest/#"&gt;imgaug&lt;/a&gt;. If you want to read more about them, look &lt;a href="http://imgaug.readthedocs.io/en/latest/source/augmenters.html"&gt;here&lt;/a&gt; . Other are just base technique from &lt;a href="http://pytorch.org/"&gt;PyTorch&lt;/a&gt; (library we used for experiments). Also I have used some open-source implementation, which we found at GitHub.&lt;br&gt;
Enough of introduction, let’s make experiments to check the influence of each model. As stated at &lt;a href="http://blcv.pl/static/2017/11/08/demystifying-face-recognition-ii-baseline/"&gt;blog-post-1&lt;/a&gt;, here we will be using &lt;code&gt;Baseline&lt;/code&gt; model without &lt;code&gt;DropOut&lt;/code&gt;, because we want to see even small difference in performance. Also, for &lt;code&gt;Random-Crop&lt;/code&gt; and &lt;code&gt;Random-Sized-Crop&lt;/code&gt; we will be using 3-different input image size with different value of padding (the bigger padding the bigger input samples space). Here are the results, where the first part of table is Affine transformation and second one Color/Lighting.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" height="460" width="210" src="http://localhost:2368/content/images/2018/02/data_aug_base.jpg"&gt;
&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var simple= {"simple_data":[{"Idx": "00", "LFW Blufr1-Rank1": "46.39%", "LFW Blufr2-FAR 0.1%": "72.95%", "Val Acc": "78.56%", "LFW": "95.71%", "Method": "No Data Aug", "Val Loss": "1.668"}, {"Idx": "01", "LFW Blufr1-Rank1": "52.94%", "LFW Blufr2-FAR 0.1%": "80.11%", "Val Acc": "81.70%", "LFW": "96.76%", "Method": "Flip", "Val Loss": "1.34"}, {"Idx": "02", "LFW Blufr1-Rank1": "49.75%", "LFW Blufr2-FAR 0.1%": "76.55%", "Val Acc": "81.71%", "LFW": "96.16%", "Method": "Random-Crop with Pad = 10", "Val Loss": "1.3827"}, {"Idx": "03", "LFW Blufr1-Rank1": "50.86%", "LFW Blufr2-FAR 0.1%": "81.45%", "Val Acc": "83.17%", "LFW": "96.71%", "Method": "Random-Crop with Pad = 20", "Val Loss": "1.226"}, {"Idx": "04", "LFW Blufr1-Rank1": "51.99%", "LFW Blufr2-FAR 0.1%": "81.43%", "Val Acc": "84.13%", "LFW": "96.73%", "Method": "Random-Crop with Pad = 30", "Val Loss": "1.2083"}, {"Idx": "05", "LFW Blufr1-Rank1": "51.29%", "LFW Blufr2-FAR 0.1%": "80.39%", "Val Acc": "83.20%", "LFW": "96.65%", "Method": "Random-Resized-Crop with Pad 10, min_scale = 0.7", "Val Loss": "1.2657"}, {"Idx": "06", "LFW Blufr1-Rank1": "49.84%", "LFW Blufr2-FAR 0.1%": "79.82%", "Val Acc": "82.89%", "LFW": "96.61%", "Method": "Random-Resized-Crop with Pad 20, min_scale = 0.7", "Val Loss": "1.2818"}, {"Idx": "07", "LFW Blufr1-Rank1": "46.51%", "LFW Blufr2-FAR 0.1%": "76.05%", "Val Acc": "80.82%", "LFW": "96.06%", "Method": "Random-Resized-Crop with Pad 30, min_scale = 0.7", "Val Loss": "1.3865"}, {"Idx": "08", "LFW Blufr1-Rank1": "55.23%", "LFW Blufr2-FAR 0.1%": "81.50%", "Val Acc": "84.56%", "LFW": "97.05%", "Method": "Random-Resized-Crop with Pad 10, min_scale = 0.5", "Val Loss": "1.1677"}, {"Idx": "09", "LFW Blufr1-Rank1": "50.82%", "LFW Blufr2-FAR 0.1%": "78.04%", "Val Acc": "80.77%", "LFW": "96.46%", "Method": "Random-Resized-Crop with Pad 10, min_scale = 0.9", "Val Loss": "1.4766"}, {"Idx": "10", "LFW Blufr1-Rank1": "51.29%", "LFW Blufr2-FAR 0.1%": "78.66%", "Val Acc": "81.43%", "LFW": "96.39%", "Method": "Random-Rotation with Pad 10, degree = 10", "Val Loss": "1.4128"}, {"Idx": "11", "LFW Blufr1-Rank1": "51.06%", "LFW Blufr2-FAR 0.1%": "78.80%", "Val Acc": "81.60%", "LFW": "96.08%", "Method": "Random-Rotation with Pad 20, degree = 10", "Val Loss": "1.403"}, {"Idx": "12", "LFW Blufr1-Rank1": "50.59%", "LFW Blufr2-FAR 0.1%": "78.12%", "Val Acc": "81.34%", "LFW": "96.31%", "Method": "Random-Rotation with Pad 0, degree = 10", "Val Loss": "1.3815"}, {"Idx": "13", "LFW Blufr1-Rank1": "49.30%", "LFW Blufr2-FAR 0.1%": "77.04%", "Val Acc": "81.56%", "LFW": "96.28%", "Method": "Random-Rotation with Pad 0, degree = 15", "Val Loss": "1.3413"}, {"Idx": "14", "LFW Blufr1-Rank1": "47.69%", "LFW Blufr2-FAR 0.1%": "75.26%", "Val Acc": "78.84%", "LFW": "95.78%", "Method": "Contrast", "Val Loss": "1.7478"}, {"Idx": "15", "LFW Blufr1-Rank1": "46.40%", "LFW Blufr2-FAR 0.1%": "73.89%", "Val Acc": "78.40%", "LFW": "96.01%", "Method": "Contrast per Channel", "Val Loss": "1.7549"}, {"Idx": "16", "LFW Blufr1-Rank1": "45.45%", "LFW Blufr2-FAR 0.1%": "71.89%", "Val Acc": "78.70%", "LFW": "95.18%", "Method": "Gaussian Noise", "Val Loss": "1.6374"}, {"Idx": "17", "LFW Blufr1-Rank1": "46.02%", "LFW Blufr2-FAR 0.1%": "73.03%", "Val Acc": "78.71%", "LFW": "95.43%", "Method": "Gaussian Noise per Channel", "Val Loss": "1.6151"}, {"Idx": "18", "LFW Blufr1-Rank1": "46.53%", "LFW Blufr2-FAR 0.1%": "74.15%", "Val Acc": "78.92%", "LFW": "95.98%", "Method": "Multiply", "Val Loss": "1.6664"}, {"Idx": "19", "LFW Blufr1-Rank1": "44.85%", "LFW Blufr2-FAR 0.1%": "73.49%", "Val Acc": "78.45%", "LFW": "95.76%", "Method": "Multiply per Channel", "Val Loss": "1.6858"}, {"Idx": "20", "LFW Blufr1-Rank1": "48.87%", "LFW Blufr2-FAR 0.1%": "73.95%", "Val Acc": "78.43%", "LFW": "95.59%", "Method": "Hue and Saturation", "Val Loss": "1.7521"}, {"Idx": "21", "LFW Blufr1-Rank1": "46.88%", "LFW Blufr2-FAR 0.1%": "74.40%", "Val Acc": "78.62%", "LFW": "95.76%", "Method": "Add", "Val Loss": "1.67"}, {"Idx": "22", "LFW Blufr1-Rank1": "45.00%", "LFW Blufr2-FAR 0.1%": "74.03%", "Val Acc": "78.54%", "LFW": "96.03%", "Method": "Add per Channel", "Val Loss": "1.6968"}, {"Idx": "23", "LFW Blufr1-Rank1": "46.97%", "LFW Blufr2-FAR 0.1%": "74.57%", "Val Acc": "78.53%", "LFW": "95.78%", "Method": "Lighting", "Val Loss": "1.76"}, {"Idx": "24", "LFW Blufr1-Rank1": "51.87%", "LFW Blufr2-FAR 0.1%": "78.28%", "Val Acc": "80.22%", "LFW": "96.44%", "Method": "Gray-Scale", "Val Loss": "1.4913"}]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="simple" class="customers"&gt;
  &lt;caption&gt; Table 1 General DA technique Results&lt;/caption&gt;
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Idx&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;Val Loss&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(4)')" style="cursor:pointer"&gt;Val Acc&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(5)')" style="cursor:pointer"&gt;LFW&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(6)')" style="cursor:pointer"&gt;LFW Blufr1-Rank1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#simple', '.item', 'td:nth-child(7)')" style="cursor:pointer"&gt;LFW Blufr2-FAR 0.1%&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="simple_data" class="item"&gt;
    &lt;td&gt;{{Idx}}&lt;/td&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{Val Loss}}&lt;/td&gt;
    &lt;td&gt;{{Val Acc}}&lt;/td&gt;
    &lt;td&gt;{{LFW}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr1-Rank1}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr2-FAR 0.1%}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("simple", simple);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;iframe width="860" height="532" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQQZNy4_JojNRXGknOCpT6EUHW3LQ8DJXW-unlhYzY1NM9gJvbXImRN5_gPMlwWaJ1SMi8eIcfatn-a/pubchart?oid=798449295&amp;amp;format=interactive"&gt;&lt;/iframe&gt;
&lt;br&gt;
&lt;p&gt;The above results are pretty intriguing. Firstly let's talk about &lt;code&gt;Affine Transformation&lt;/code&gt;. Look like using any of them provide better results than doing nothing (so good news, DA doesn't hurt model). However looks like there are very sensitive to hyperparameters (like Padding or MinScale). If we would just take random parameters, we could even get worse model than our baseline. It is also worth mentioning that simple &lt;code&gt;Horizontal-Flip&lt;/code&gt; is really powerful technique, we didn't expect that! The best &lt;code&gt;Affine model&lt;/code&gt; is &lt;code&gt;Random-Resized-Crop&lt;/code&gt; with Pad = 10 and MinScale = 0.5.&lt;br&gt;
The story about &lt;code&gt;Color Perturbation&lt;/code&gt; is completely different. Here hardly any technique get much better results than &lt;code&gt;No Data Augmentation&lt;/code&gt;. The only one is &lt;code&gt;Grayscale&lt;/code&gt;. We are not sure if our test are correctly carried out, however even some hyper-parameter search doesn't improve performance.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id="recentlyintroducetechnique"&gt;Recently Introduce Technique&lt;/h2&gt;
&lt;p&gt;In 2017 the two new data-based regularization technique was introduced:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1708.04896"&gt;Random-Erasing&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/1708.04552"&gt;CutOut&lt;/a&gt;: removing the random rectangular area from image&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1710.09412"&gt;MixUp&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/1801.02929"&gt;Image-Pairing&lt;/a&gt;: mixing values two images (and targets in MixUp) with each other&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
Removing the values from input image was a popular method in the begging of Deep Learning, it was done by using DropOut as a first layer (it was introduced at Denoising AutoEncoder). But when using convolution layers, it does not work well. It is believed that it is because of Kernel-Idea in CNN, so removing the single values of area does not make a big impact of the final output. When removing the bigger area than kernels then it would make a much bigger influence in final model. This is exact idea behind `RandomErasing`. We also included technique from imgaug library, `DropOut` and `CorseDropOut` (which is sth like `Random Erasing`) with Channel-dependent and independent version.
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" height="460" width="210" src="https://www.doc.ic.ac.uk/~js4416/163/website/img/autoencoders/denoising-example.png"&gt;
&lt;/p&gt;
&lt;p&gt;In the other hand, &lt;code&gt;Mixup&lt;/code&gt; is  novel idea in which many researcher does not believe it would work. How does mixing the image intensities and target can regularize the model? How does model can learn that this image is 80% of Leonardo DiCaprio and 20% Tom Hanks? For us it is really intriguing, but it may be connected with prediction distribution, act like a regulizer. Just look at the example images from ImageNet created using this technique, could you classify them correctly.&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="pl"&gt;&lt;p lang="en" dir="ltr"&gt;Here are a couple of Imagenet photos with different blending ratios. They are harder to categorize but it is not impossible! cc &lt;a href="https://twitter.com/ogrisel?ref_src=twsrc%5Etfw"&gt;@ogrisel&lt;/a&gt; &lt;a href="https://t.co/cqqBVUC0qG"&gt;pic.twitter.com/cqqBVUC0qG&lt;/a&gt;&lt;/p&gt;&amp;mdash; Xavier Gastaldi (@xavier_gastaldi) &lt;a href="https://twitter.com/xavier_gastaldi/status/925952887016099840?ref_src=twsrc%5Etfw"&gt;2 listopada 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
&lt;p&gt;This idea remind us two technique which was developed to make labels noise: Label-Smoothing (&lt;a href="https://arxiv.org/abs/1512.00567"&gt;Rethinking the Inception Architecture for Computer Vision&lt;/a&gt;) and Label-corruption. Also blending the data is not a new idea, &lt;a href="https://www.jair.org/media/953/live-953-2037-jair.pdf"&gt;SMOTE&lt;/a&gt;  algorithm was doing exactly the same situation, but it was rather designed for oversampling the minority classes, so was just merging examples (even more than 2) of the same class (what based on the author of mixup paper, does not work well). It look like that mixup is sth like creating Multi-class SMOTE with Label Smoothing. Image-Pairing, in contrast to MixUp, does not merge labels information, just image intensities. In this technique is single hyper-parameter, alpha, which is float number in range &amp;lt;0,1&amp;gt; indicating proportion of intersities taken from both images. If you would like to understand deeper this technique, we recommend great blog by &lt;a href="http://www.inference.vc/mixup-data-dependent-data-augmentation/"&gt;inFERENCe&lt;/a&gt;. How it works with Faces?&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" height="500" width="500" src="https://www.researchgate.net/publication/287601878/figure/fig1/AS:316826589384744@1452548753581/The-schematic-of-NRSBoundary-SMOTE-algorithm.png"&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" height="600" width="480" src="http://localhost:2368/content/images/2018/02/data_aug_erase.jpg"&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" height="600" width="480" src="http://localhost:2368/content/images/2018/02/blog_5_mixup.jpg"&gt;
&lt;/p&gt;
&lt;p&gt;Before making the experiments, sets clarify hyper-parameters of methods. In case of &lt;code&gt;RandomErasing&lt;/code&gt;, we will be make several experiments making probability of using it different and also choosing the maximum size of removed area. Using &lt;code&gt;Mixup&lt;/code&gt; only just need to set a range of ratio between two training examples. Here are results.&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var mix= {"mix_data":[{"Idx": "00", "LFW Blufr1-Rank1": "47.55%", "LFW Blufr2-FAR 0.1%": "74.38%", "Val Acc": "80.07%", "LFW": "96.08%", "Method": "image-pairing α = 0.2", "Val Loss": "1.4646"}, {"Idx": "01", "LFW Blufr1-Rank1": "48.05%", "LFW Blufr2-FAR 0.1%": "75.29%", "Val Acc": "79.86%", "LFW": "96.08%", "Method": "mixup α = 0.05", "Val Loss": "1.49"}, {"Idx": "02", "LFW Blufr1-Rank1": "49.26%", "LFW Blufr2-FAR 0.1%": "74.82%", "Val Acc": "80.59%", "LFW": "96.05%", "Method": "mixup α = 0.1", "Val Loss": "1.43"}, {"Idx": "03", "LFW Blufr1-Rank1": "48.13%", "LFW Blufr2-FAR 0.1%": "74.77%", "Val Acc": "80.38%", "LFW": "96.34%", "Method": "mixup α = 0.2", "Val Loss": "1.44"}, {"Idx": "04", "LFW Blufr1-Rank1": "48.68%", "LFW Blufr2-FAR 0.1%": "76.12%", "Val Acc": "80.65%", "LFW": "96.08%", "Method": "mixup α = 0.4", "Val Loss": "1.43"}, {"Idx": "05", "LFW Blufr1-Rank1": "48.11%", "LFW Blufr2-FAR 0.1%": "75.88%", "Val Acc": "80.22%", "LFW": "95.98%", "Method": "mixup α = 0.7", "Val Loss": "1.47"}, {"Idx": "06", "LFW Blufr1-Rank1": "45.41%", "LFW Blufr2-FAR 0.1%": "73.85%", "Val Acc": "80.91%", "LFW": "95.83%", "Method": "Random-Erasing with p = 0.8, imagenet", "Val Loss": "1.4095"}, {"Idx": "07", "LFW Blufr1-Rank1": "49.79%", "LFW Blufr2-FAR 0.1%": "76.17%", "Val Acc": "81.20%", "LFW": "96.23%", "Method": "Random-Erasing with p = 0.5", "Val Loss": "1.3407"}, {"Idx": "08", "LFW Blufr1-Rank1": "49.28%", "LFW Blufr2-FAR 0.1%": "75.82%", "Val Acc": "82.27%", "LFW": "96.49%", "Method": "Random-Erasing with p = 0.8", "Val Loss": "1.3014"}, {"Idx": "09", "LFW Blufr1-Rank1": "50.39%", "LFW Blufr2-FAR 0.1%": "76.57%", "Val Acc": "82.39%", "LFW": "96.25%", "Method": "Random-Erasing with p = 1.0", "Val Loss": "1.3172"}, {"Idx": "10", "LFW Blufr1-Rank1": "48.02%", "LFW Blufr2-FAR 0.1%": "75.35%", "Val Acc": "79.15%", "LFW": "95.36%", "Method": "Dropout", "Val Loss": "1.4825"}, {"Idx": "11", "LFW Blufr1-Rank1": "46.95%", "LFW Blufr2-FAR 0.1%": "74.61%", "Val Acc": "79.57%", "LFW": "95.85%", "Method": "Dropout per Channel", "Val Loss": "1.4653"}, {"Idx": "12", "LFW Blufr1-Rank1": "49.39%", "LFW Blufr2-FAR 0.1%": "75.89%", "Val Acc": "80.97%", "LFW": "95.85%", "Method": "Coarse Dropout", "Val Loss": "1.3796"}, {"Idx": "13", "LFW Blufr1-Rank1": "48.13%", "LFW Blufr2-FAR 0.1%": "74.05%", "Val Acc": "79.98%", "LFW": "96.03%", "Method": "Coarse Dropout per Channel", "Val Loss": "1.4614"}]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="mix" class="customers"&gt;
   &lt;caption&gt; Table 2 Mixing/Dropping technique Results&lt;/caption&gt;
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Idx&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;Val Loss&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(4)')" style="cursor:pointer"&gt;Val Acc&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(5)')" style="cursor:pointer"&gt;LFW&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(6)')" style="cursor:pointer"&gt;LFW Blufr1-Rank1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#mix', '.item', 'td:nth-child(7)')" style="cursor:pointer"&gt;LFW Blufr2-FAR 0.1%&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="mix_data" class="item"&gt;
    &lt;td&gt;{{Idx}}&lt;/td&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{Val Loss}}&lt;/td&gt;
    &lt;td&gt;{{Val Acc}}&lt;/td&gt;
    &lt;td&gt;{{LFW}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr1-Rank1}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr2-FAR 0.1%}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("mix", mix);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;iframe width="789.5" height="488.1741666666667" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQQZNy4_JojNRXGknOCpT6EUHW3LQ8DJXW-unlhYzY1NM9gJvbXImRN5_gPMlwWaJ1SMi8eIcfatn-a/pubchart?oid=824193697&amp;amp;format=interactive"&gt;&lt;/iframe&gt;
&lt;p&gt;Before making any summary about results, we would like to point out our problems during making the experiments of &lt;code&gt;Image Pairing&lt;/code&gt; idea. According to their paper and &lt;code&gt;inFERENCe&lt;/code&gt; blog post, there is no need of smoothing the labels, input data is enough. According to some theoretical analyse, it is true. But our finding are different when the training procedure is exactly the same for &lt;code&gt;MixUp&lt;/code&gt; and &lt;code&gt;Image-Pairing&lt;/code&gt;. The main difference in training procedure in &lt;code&gt;Image-Pairing&lt;/code&gt; is turning on/off it during training (ex. first 3 epoch off, then 5 on, 3 off etc.). With such setting is converge to similar results like &lt;code&gt;MixUp&lt;/code&gt;, but without such methodology it diverge. So we advice to use &lt;code&gt;MixUp&lt;/code&gt; in any time you want check this kind of technique.&lt;/p&gt;
&lt;p&gt;Ok, but how good are this technique in Face-Recognition? Many of them works really well compared to baseline model. They even achieve better validation score than &lt;code&gt;Random-Flip&lt;/code&gt;. However, the situation at &lt;code&gt;LFW-BLUFR&lt;/code&gt; test is different. Their results are much lower even they obtain lower &lt;code&gt;Validation Loss&lt;/code&gt; (&lt;code&gt;Random-Erasing with p = 1.0&lt;/code&gt; vs &lt;code&gt;Random-Erasing with p = 1.0&lt;/code&gt;). What does it mean? Look like features representation from network learned with &lt;code&gt;missing data&lt;/code&gt; are not so good (or maybe it is wrong conclusion?). This is one of the problem of Face-Recognition, measure for &lt;code&gt;quality of features&lt;/code&gt; different than just accuracy in some benchmarks (we will talk about this later).&lt;/p&gt;
&lt;h2 id="facespecificdataaugmentationtechnique"&gt;Face-Specific Data-Augmentation technique&lt;/h2&gt;
&lt;p&gt;Look like that we are able to get a nice boost using general Data Augmentation technique. But this is not the end. In many years of developing Face-Recognition technique, there are also proposed Face-Specific method, which could work only with them. We know two of them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Landmark perturbation: this cause that image alignment is not perfect&lt;/li&gt;
&lt;li&gt;Face-Rendering: rendering the novel views of Face with different Pose&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, our earlier experiment in &lt;a href="http://blcv.pl/static/2017/11/08/demystifying-face-recognition-ii-baseline/"&gt;Face-Align&lt;/a&gt;, show that accuracy of landmark location didn’t influence the final accuracy. But what if Face Alignment would be not perfect/different every time the model see the image? Having such technique would be equivalent to rotation-resizing-cropping at once, but with much smaller range of values. Just look into below images.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" height="600" width="480" src="http://localhost:2368/content/images/2018/02/blog5-land.jpg"&gt;
&lt;/p&gt;
&lt;p&gt;Face-Rendering is much harder topic. In it connected with estimating the 3D-pose of head and generating the new views of face. Based on paper &lt;a href="https://arxiv.org/abs/1708.07517"&gt;FacePoseNet&lt;/a&gt;, it works really well, when 3D pose is estimated very well (using landmarks provide much lower accuracy). However, we have issues with this technique, so we would not conduct extensive experiments using it now (planed it future).&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var land= {"land_data":[{"Idx": "00", "LFW Blufr1-Rank1": "54.22%", "LFW Blufr2-FAR 0.1%": "79.99%", "Val Acc": "82.35%", "LFW": "96.85%", "Method": "Landmark-Perturbation σ = 3", "Val Loss": "1.383"}, {"Idx": "01", "LFW Blufr1-Rank1": "54.90%", "LFW Blufr2-FAR 0.1%": "81.34%", "Val Acc": "83.54%", "LFW": "97.10%", "Method": "Landmark-Perturbation σ = 5", "Val Loss": "1.2692"}, {"Idx": "02", "LFW Blufr1-Rank1": "54.70%", "LFW Blufr2-FAR 0.1%": "80.66%", "Val Acc": "84.03%", "LFW": "97.01%", "Method": "Landmark-Perturbation σ = 7", "Val Loss": "1.2"}]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="land" class="customers"&gt;
   &lt;caption&gt; Table 3 Landmark-Perturbation technique Results&lt;/caption&gt; 
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Idx&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;Val Loss&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(4)')" style="cursor:pointer"&gt;Val Acc&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(5)')" style="cursor:pointer"&gt;LFW&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(6)')" style="cursor:pointer"&gt;LFW Blufr1-Rank1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#land', '.item', 'td:nth-child(7)')" style="cursor:pointer"&gt;LFW Blufr2-FAR 0.1%&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="land_data" class="item"&gt;
    &lt;td&gt;{{Idx}}&lt;/td&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{Val Loss}}&lt;/td&gt;
    &lt;td&gt;{{Val Acc}}&lt;/td&gt;
    &lt;td&gt;{{LFW}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr1-Rank1}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr2-FAR 0.1%}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("land", land);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;iframe width="799.5" height="494.3575000000001" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQQZNy4_JojNRXGknOCpT6EUHW3LQ8DJXW-unlhYzY1NM9gJvbXImRN5_gPMlwWaJ1SMi8eIcfatn-a/pubchart?oid=1918168880&amp;amp;format=interactive"&gt;&lt;/iframe&gt;
&lt;p&gt;Really nice results! There are comparable to best &lt;code&gt;Affine&lt;/code&gt; technique already tested.&lt;/p&gt;
&lt;h2 id="combinationoftechnique"&gt;Combination of technique&lt;/h2&gt;
&lt;p&gt;As we know how each single technique works, let's now combine them into more complicated form. The chosen hyperparameter (like type of combination) are just our intuition, there are maybe not best choose. However we wanted to test wide-range of possibilities, where our main aim was creating the best possible model.&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var merge_da= {"customers_data":[{"Idx": "00", "LFW Blufr1-Rank1": "50.59%", "LFW Blufr2-FAR 0.1%": "78.43%", "Val Acc": "83.86%", "LFW": "96.26%", "Method": "flip-grayscale-erasing", "Val Loss": "1.2096"}, {"Idx": "01", "LFW Blufr1-Rank1": "55.90%", "LFW Blufr2-FAR 0.1%": "86.07%", "Val Acc": "85.24%", "LFW": "97.50%", "Method": "flip-crop30", "Val Loss": "1.1083"}, {"Idx": "02", "LFW Blufr1-Rank1": "51.85%", "LFW Blufr2-FAR 0.1%": "82.37%", "Val Acc": "84.94%", "LFW": "97.21%", "Method": "flip-resize05_pad10", "Val Loss": "1.1251"}, {"Idx": "03", "LFW Blufr1-Rank1": "58.17%", "LFW Blufr2-FAR 0.1%": "84.80%", "Val Acc": "84.70%", "LFW": "97.23%", "Method": "flip-landmark_05", "Val Loss": "1.1556"}, {"Idx": "04", "LFW Blufr1-Rank1": "56.53%", "LFW Blufr2-FAR 0.1%": "82.17%", "Val Acc": "85.58%", "LFW": "97.28%", "Method": "landmark_05-crop20", "Val Loss": "1.132"}, {"Idx": "05", "LFW Blufr1-Rank1": "55.79%", "LFW Blufr2-FAR 0.1%": "82.52%", "Val Acc": "84.85%", "LFW": "97.41%", "Method": "landmark_05-resize_05_pad10", "Val Loss": "1.1694"}, {"Idx": "06", "LFW Blufr1-Rank1": "56.73%", "LFW Blufr2-FAR 0.1%": "85.12%", "Val Acc": "86.31%", "LFW": "97.33%", "Method": "flip-landmark_05-crop20", "Val Loss": "1.0478"}, {"Idx": "07", "LFW Blufr1-Rank1": "56.11%", "LFW Blufr2-FAR 0.1%": "85.36%", "Val Acc": "86.27%", "LFW": "97.65%", "Method": "flip-landmark_05-crop20-grayscale", "Val Loss": "1.0583"}, {"Idx": "08", "LFW Blufr1-Rank1": "54.86%", "LFW Blufr2-FAR 0.1%": "85.02%", "Val Acc": "87.51%", "LFW": "97.39%", "Method": "flip-landmark_05-crop20-mixup_02", "Val Loss": "0.9822"}, {"Idx": "09", "LFW Blufr1-Rank1": "55.99%", "LFW Blufr2-FAR 0.1%": "84.50%", "Val Acc": "87.15%", "LFW": "97.53%", "Method": "flip-landmark_05-crop20-erase_08", "Val Loss": "1.005"}, {"Idx": "10", "LFW Blufr1-Rank1": "64.72%", "LFW Blufr2-FAR 0.1%": "90.58%", "Val Acc": "88.82%", "LFW": "98.53%", "Method": "flip-landmark_05-crop20-linear_dropout_05", "Val Loss": "0.92"}, {"Idx": "11", "LFW Blufr1-Rank1": "67.14%", "LFW Blufr2-FAR 0.1%": "91.30%", "Val Acc": "89.20%", "LFW": "98.19%", "Method": "flip-landmark_05-linear_dropout_05", "Val Loss": "0.9065"}, {"Idx": "12", "LFW Blufr1-Rank1": "65.74%", "LFW Blufr2-FAR 0.1%": "91.12%", "Val Acc": "88.47%", "LFW": "98%", "Method": "flip-crop20-linear_dropout_05", "Val Loss": "0.9381"}, {"Idx": "13", "LFW Blufr1-Rank1": "61.08%", "LFW Blufr2-FAR 0.1%": "89.86%", "Val Acc": "88.05%", "LFW": "97.98%", "Method": "flip-resize05_pad10-linear_dropout_05", "Val Loss": "0.9673"},{"Idx": "14", "LFW Blufr1-Rank1": "64.39%", "LFW Blufr2-FAR 0.1%": "89.11%", "Val Acc": "86.9%", "LFW": "97.76%", "Method": "flip-linear_dropout_05", "Val Loss": "1.02"}]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="merge" class="customers"&gt;
  &lt;caption&gt; Table 4 Multiple DA technique&lt;/caption&gt;
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Idx&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;Val Loss&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(4)')" style="cursor:pointer"&gt;Val Acc&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(5)')" style="cursor:pointer"&gt;LFW&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(6)')" style="cursor:pointer"&gt;LFW Blufr1-Rank1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#merge', '.item', 'td:nth-child(7)')" style="cursor:pointer"&gt;LFW Blufr2-FAR 0.1%&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="customers_data" class="item"&gt;
    &lt;td&gt;{{Idx}}&lt;/td&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{Val Loss}}&lt;/td&gt;
    &lt;td&gt;{{Val Acc}}&lt;/td&gt;
    &lt;td&gt;{{LFW}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr1-Rank1}}&lt;/td&gt;
    &lt;td&gt;{{LFW Blufr2-FAR 0.1%}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("merge", merge_da);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;iframe width="754" height="466" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQQZNy4_JojNRXGknOCpT6EUHW3LQ8DJXW-unlhYzY1NM9gJvbXImRN5_gPMlwWaJ1SMi8eIcfatn-a/pubchart?oid=413561002&amp;amp;format=interactive"&gt;&lt;/iframe&gt;
&lt;h5 id="flipgrayscaleerasingidx0"&gt;Flip-grayscale-erasing (Idx: 0)&lt;/h5&gt;
&lt;p&gt;This was our test which tied to combine &lt;code&gt;average&lt;/code&gt; DA technique to create more powerful method. And results are disappoints, despite having nice &lt;code&gt;Validation&lt;/code&gt; results, &lt;code&gt;LFW&lt;/code&gt; benchmarks showed that features are even worse form the case of using just &lt;code&gt;Flip&lt;/code&gt;. Maybe this is because of erasing?&lt;/p&gt;
&lt;h5 id="combinationof2affinemethodidx15"&gt;Combination of 2 Affine method (Idx: 1-5)&lt;/h5&gt;
&lt;p&gt;Our next idea was just combining the best DA technique and see if they work even better or are redundant. And look like that combining them works pretty nice. &lt;code&gt;Flip&lt;/code&gt; boost accuracy of 'Cropping&lt;code&gt;and&lt;/code&gt;Landmark-Perturbation' (currently two best methods), in the other hand 'ResizeCrop&lt;code&gt;got comparable results in 'Validation&lt;/code&gt; and much worse in &lt;code&gt;LFW&lt;/code&gt; protocol. In case of testing &lt;code&gt;Landmark-Perturbation&lt;/code&gt; with 'Cropping&lt;code&gt;and 'ResizeCrop&lt;/code&gt;, both of them get just some of boost. As we said earlier, &lt;code&gt;Landmark-Perturbation&lt;/code&gt; already contain multiple  &lt;code&gt;Affine&lt;/code&gt; technique, so maybe this collaboration is redundant.&lt;/p&gt;
&lt;h5 id="getbestpossiblemodelidx69"&gt;Get best possible model (Idx: 6-9)&lt;/h5&gt;
&lt;p&gt;Here we decided to use base method &lt;code&gt;Flip-landmark_05-crop20&lt;/code&gt; with pretty decent results and some non-redundant methods, exactly: &lt;code&gt;Mixup&lt;/code&gt;, &lt;code&gt;Random-Erase&lt;/code&gt; and &lt;code&gt;GrayScale&lt;/code&gt;. And all the method get comparable or better &lt;code&gt;Validation&lt;/code&gt; results than our &lt;code&gt;baseline&lt;/code&gt; model (idx:14). Unfortunately in &lt;code&gt;LFW BLUFR&lt;/code&gt; protocol there is huge gap between results. This just bring us again to thought about meaningful of feature representation, how we should measure it correctly as again and again &lt;code&gt;Validation Loss&lt;/code&gt; doesn't give pure indication about that.&lt;/p&gt;
&lt;h5 id="modelswithdropoutidx1014"&gt;Models with DropOut (Idx: 10-14)&lt;/h5&gt;
&lt;p&gt;Look like &lt;code&gt;DropOut&lt;/code&gt; technique would stay with us as the regularization applied by Data Augmentation have the different effect on final feature representation. So we just tried several option, however this time we use &lt;code&gt;FaceResNet-DropOut&lt;/code&gt;. And we have new kind, &lt;code&gt;Flip-landmark_05-linear_dropout_05&lt;/code&gt;! This model is our currently best model overall, it even beat much more complicated models like SphereFace. Here are results from &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Klare_Pushing_the_Frontiers_2015_CVPR_paper.pdf"&gt;IJBA&lt;/a&gt; and &lt;a href="http://megaface.cs.washington.edu/"&gt;MegaFace&lt;/a&gt; benchmarks, compared to our best model using same architecture.&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var ijba= {"ijba_data":[{"TAR@FAR 0.01%": "53.04±5.37", "TAR@FAR 0.1%": "71.90±3.02", "TAR@FAR 1%": "84.45±1.42", "TAR@FAR 10%": "93.14±0.77", "Method": "Flip-linear_dropout", "Rank-1": "91.37±0.95", "Rank-5": "95.71±0.62"}, {"TAR@FAR 0.01%": "56.02±8.99", "TAR@FAR 0.1%": "73.81±4.34", "TAR@FAR 1%": "86.73±1.48", "TAR@FAR 10%": "95.23±0.67", "Method": "Flip-landmark-linear_dropout", "Rank-1": "92.24±1.10", "Rank-5": "96.56±0.56"}]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="ijba" class="customers"&gt;
    &lt;caption&gt; Table 5 IJBA-A Results&lt;/caption&gt;
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;TAR@FAR 0.01%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;TAR@FAR 0.1%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(4)')" style="cursor:pointer"&gt;TAR@FAR 1%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(5)')" style="cursor:pointer"&gt;TAR@FAR 10%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(6)')" style="cursor:pointer"&gt;Rank-1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba', '.item', 'td:nth-child(7)')" style="cursor:pointer"&gt;Rank-5&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="ijba_data" class="item"&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 0.01%}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 0.1%}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 1%}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 10%}}&lt;/td&gt;
    &lt;td&gt;{{Rank-1}}&lt;/td&gt;
    &lt;td&gt;{{Rank-5}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("ijba", ijba);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;br&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var megaface= {"megaface_data":[{"Method": "Flip-linear_dropout", "Rank-1": "53.55%", "VR@FAR=10−6": "61.80%"}, {"Method": "Flip-landmark-linear_dropout", "Rank-1": "55.27%", "VR@FAR=10−6": "61.42%"}]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="megaface" class="customers"&gt;
   &lt;caption&gt; Table 6 MegaFace Results&lt;/caption&gt;
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#megaface', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#megaface', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;Rank-1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#megaface', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;VR@FAR=10−6&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="megaface_data" class="item"&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{Rank-1}}&lt;/td&gt;
    &lt;td&gt;{{VR@FAR=10−6}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("megaface", megaface);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;br&gt;
&lt;h2 id="summaryofdataaugmentation"&gt;Summary of Data-Augmentation&lt;/h2&gt;
&lt;p&gt;Here are all the tests we have done. There are a lot of information here, we will try to summarize them by mainly looking into &lt;code&gt;Validation&lt;/code&gt; results. We would order them by usefulness in general Computer Vision tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Affine Image transformation are really powerful method, it is recommended to try &lt;code&gt;Cropping, Scaling, Rotating&lt;/code&gt; or all at once at every task you have.&lt;/li&gt;
&lt;li&gt;Coarse-Dropout/Random-Erasing are powerful technique, which really can help you creating best-possible model.&lt;/li&gt;
&lt;li&gt;Mixing images intensities is very strong regulator. In general, it can help you to get better model, only if your model is really powerful (as stated in original &lt;a href="https://arxiv.org/abs/1710.09412"&gt;paper&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Color Transformation doesn't provide huge boost, but this doesn't preclude them for using. Just doesn't expect huge boost.&lt;/li&gt;
&lt;li&gt;Merging: merging couple of different/same type of technique provide better results, however it need to be treat as hyper-parameter.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, Data-Augmentation regularization shouldn't discourage using other technique like DropOut or Weight-Decay, so you try them also.&lt;/p&gt;
&lt;h2 id="noteaboutfeaturerepresentation"&gt;Note about Feature-Representation&lt;/h2&gt;
&lt;p&gt;Our last topic in this blog post will be some notes about &lt;code&gt;Feature-Representation&lt;/code&gt;. Generally, most of the researcher agreed with argument that: &lt;code&gt;The Better Accuracy you achieved in database, the better representation is learned by model&lt;/code&gt;. Then, creating better models/learning methods for achieving better accuracy score should also provide better representation. We have also problem with such formulation if we would take a look of idea of testing &lt;code&gt;Face-Recognition&lt;/code&gt; technology. Let's just look into idea how we are testing the real performance of model.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" src="http://localhost:2368/content/images/2018/02/last_layer.jpg" width="500" height="380"&gt;
&lt;/p&gt;
&lt;p&gt;Optimizing the final accuracy, we are doesn't care us much about &lt;code&gt;pre-final-layer&lt;/code&gt;, it is optimized in similar fashion like other layer despite the fast it is most importart part of all model. This is why currenty is visible trend in &lt;code&gt;Face Recognition, Image Retrival&lt;/code&gt; technology to create task related loss functions, like mentioned several time &lt;a href="https://ydwen.github.io/papers/WenECCV16.pdf"&gt;CenterLoss&lt;/a&gt; or &lt;a href="http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf"&gt;Contrastive loss&lt;/a&gt;, because they directly interact with final representation of model. We will more about this topic in future blog-post, but we just want to give brief overview of our doubts in general arguments, which we already know.&lt;/p&gt;
&lt;p&gt;However, the main topic of this section should be the new findings from series of experiments. Generally, in contrast to our &lt;a href="http://blcv.pl/static/2017/11/08/demystifying-face-recognition-ii-baseline/"&gt;baseline experiments&lt;/a&gt;, where more powerfull structure provide better Accuray and Representation, in our experiments is was not always the truth. Generally, the biggest problem occur with using Mixing/Dropping technology, which was providing deformed face to models. From our experiments here better final Accuracy doesn't mean always better Representation. Let's analyse relation between &lt;code&gt;Validation Accuracy&lt;/code&gt; and &lt;code&gt;LFW&lt;/code&gt; score.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition V: Data Augmentation" src="http://localhost:2368/content/images/2018/02/blog5-feature-depend.png" width="500" height="500"&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;table id="corr" class="customers"&gt;
  &lt;tr&gt;
    &lt;th style="cursor:pointer"&gt;Affine&lt;/th&gt;
    &lt;th style="cursor:pointer"&gt;Color&lt;/th&gt;
    &lt;th style="cursor:pointer"&gt;Mix-drop&lt;/th&gt;
    &lt;th style="cursor:pointer"&gt;Landmark&lt;/th&gt;
    &lt;th style="cursor:pointer"&gt;Merge&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr class="item"&gt;
    &lt;td&gt;0.79&lt;/td&gt;
    &lt;td&gt;0.57&lt;/td&gt;
    &lt;td&gt;0.67&lt;/td&gt;
    &lt;td&gt;0.79&lt;/td&gt;
    &lt;td&gt;0.86&lt;/td&gt;
  &lt;/tr&gt;
   &lt;caption&gt; Table 7 Correlation Val Acc vs LFW&lt;/caption&gt;
&lt;/table&gt;
&lt;p&gt;Looking into chart, there is general trend of having higher &lt;code&gt;LFW&lt;/code&gt; score with higher &lt;code&gt;Validation Accuracy&lt;/code&gt;. In the other hand when we look closer into chart categories and also correlation values, something unusual can be spotted: &lt;code&gt;Color&lt;/code&gt; and &lt;code&gt;Mix/Drop&lt;/code&gt; transformation have weaker effect on &lt;code&gt;LFW&lt;/code&gt; score than other kind of transformation. So we can deduce, that &lt;code&gt;Feature Representation&lt;/code&gt; is also less powerful compared to &lt;code&gt;Affine&lt;/code&gt; technique having same &lt;code&gt;Validation Accuracy&lt;/code&gt;. What does it mean? We are not able to make any clear statement, however look like using distorted/unnatural input image like a training example makes features of normal images less representative. But what if we would test such method in much harder environment, like video-input, as &lt;code&gt;LFW&lt;/code&gt; images are really nice. In contrast, &lt;code&gt;IJBA-A&lt;/code&gt; also use frames from video for testing. This is why we conduct several experiment to see if maybe this benchmark show us different &lt;code&gt;trendline&lt;/code&gt; for our dubious quality methods. We use &lt;code&gt;Flip-landmark_05-crop20&lt;/code&gt; as a baseline.&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;script&gt;
var ijba_feat= {"ijba_feat_data":[{"TAR@FAR 0.01%": "51.68±6.61", "TAR@FAR 0.1%": "66.64±4.94", "TAR@FAR 1%": "82.56±1.78", "TAR@FAR 10%": "94.15±0.71", "Method": "Baseline", "Rank-1": "90.03±1.11", "Rank-5": "95.80±0.72"}, {"TAR@FAR 0.01%": "52.97±10.75", "TAR@FAR 0.1%": "72.66±5.34", "TAR@FAR 1%": "87.33±1.18", "TAR@FAR 10%": "95.92±0.63", "Method": "+linear_dropout", "Rank-1": "92.44±1.47", "Rank-5": "96.79±0.66"}, 
 {"TAR@FAR 0.01%": "47.02±9.98", "TAR@FAR 0.1%": "65.07±4.48", "TAR@FAR 1%": "82.17±1.74", "TAR@FAR 10%": "94.02±0.83", "Method": "+erasing", "Rank-1": "90.48±1.32", "Rank-5": "96.07±0.65"},
  {"TAR@FAR 0.01%": "36.10±10.91", "TAR@FAR 0.1%": "56.36±6.76", "TAR@FAR 1%": "86.73±1.48", "TAR@FAR 10%": "95.23±0.67", "Method": "+mixup", "Rank-1": "92.24±1.10", "Rank-5": "96.56±0.56"},
  {"TAR@FAR 0.01%": "41.92±10.69", "TAR@FAR 0.1%": "64.19±4.39", "TAR@FAR 1%": "82.32±1.52", "TAR@FAR 10%": "94.00±0.68", "Method": "+grayscale", "Rank-1": "90.16±1.21", "Rank-5": "95.62±0.74"}
    ]
};
&lt;/script&gt;
&lt;body&gt;
&lt;table id="ijba_feat" class="customers"&gt;
    &lt;caption&gt; Table 8 IJBA-A Results&lt;/caption&gt;
  &lt;tr&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(1)')" style="cursor:pointer"&gt;Method&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(2)')" style="cursor:pointer"&gt;TAR@FAR 0.01%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(3)')" style="cursor:pointer"&gt;TAR@FAR 0.1%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(4)')" style="cursor:pointer"&gt;TAR@FAR 1%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(5)')" style="cursor:pointer"&gt;TAR@FAR 10%&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(6)')" style="cursor:pointer"&gt;Rank-1&lt;/th&gt;
    &lt;th onclick="w3.sortHTML('#ijba_feat', '.item', 'td:nth-child(7)')" style="cursor:pointer"&gt;Rank-5&lt;/th&gt;
  &lt;/tr&gt;
   &lt;tr w3-repeat="ijba_feat_data" class="item"&gt;
    &lt;td&gt;{{Method}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 0.01%}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 0.1%}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 1%}}&lt;/td&gt;
    &lt;td&gt;{{TAR@FAR 10%}}&lt;/td&gt;
    &lt;td&gt;{{Rank-1}}&lt;/td&gt;
    &lt;td&gt;{{Rank-5}}&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;script&gt;
    w3.displayObject("ijba_feat", ijba_feat);
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;br&gt;
&lt;p&gt;This experiment has helped to see a wider perspective of analysed situation. Using Non-Affine transformation enable to get better &lt;code&gt;Identification&lt;/code&gt; score and worse &lt;code&gt;Verification&lt;/code&gt; score. Ex. &lt;code&gt;MixUp&lt;/code&gt; is second best in &lt;code&gt;Identification&lt;/code&gt; and the worst (with very big gap) in &lt;code&gt;Verification&lt;/code&gt;. We have meet similar situation in the past, at &lt;a href="http://blcv.pl/static/2017/11/08/demystifying-face-recognition-ii-baseline/"&gt;Blog-1-Baseline&lt;/a&gt;,  &lt;a href="http://www.yugangjiang.info/publication/icmr17-face.pdf"&gt;Fudan Architecture&lt;/a&gt; have similar situation. We thought that it is because &lt;code&gt;Batch Normalization&lt;/code&gt;, but maybe there is other reason? We would like to explain this phenomenon, but currently we are lack of ideas. We need to find the difference coming from testing. The &lt;code&gt;Identification&lt;/code&gt; protocol is based on &lt;code&gt;kNN&lt;/code&gt; algorithm, find the closest &lt;code&gt;Face&lt;/code&gt; in dataset. &lt;code&gt;Verification&lt;/code&gt; protocol is based on finding best Threshold for given rules (best Accuracy or False-Accept rate value). So, imagining that we need best possible Face-Recognition system, we need high &lt;code&gt;Identification&lt;/code&gt; (for recognizing) and &lt;code&gt;Verification&lt;/code&gt; (for saying: &lt;code&gt;Not Recognized&lt;/code&gt;) accuracy.&lt;br&gt;
Here we would like to mention about last topic related to measurement of &lt;code&gt;Quality of Representation of features&lt;/code&gt;. From our perspective, this topic is not widely studied, some of the measurements come from testing Accuracy. However, we see currently a trend of changing it. Even &lt;code&gt;CenterLoss&lt;/code&gt; is somehow the measure compactness of features. Recently, the authors of &lt;a href="https://arxiv.org/abs/1704.08063"&gt;SphereFace&lt;/a&gt; presented their method of measuring the quality of features (&lt;code&gt;Angular Fisher score&lt;/code&gt;), which we see as extension of Center idea but with added interaction between classes. We think that it is great idea and we are thinking about adding such measurement for out comparison. More about &lt;code&gt;Quality of Features&lt;/code&gt; we will talk analysing the available Loss function, because they are currently the main influencer of final accuracy,&lt;/p&gt;
&lt;p&gt;Thanks for reading, any feedback is welcome!&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition IV: Face-Alignment</title><description>How does Face Alignment effect the accuracy of Face Recognition algorithm?</description><link>http://localhost:2368/2017/12/28/demystifying-face-recognition-iii-face-preprocessing/</link><guid isPermaLink="false">5a0fe47acf6e101744f5960a</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Thu, 28 Dec 2017 09:55:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/deepface_logo-1.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2017/11/deepface_logo-1.png" alt="Demystifying Face Recognition IV: Face-Alignment"&gt;&lt;p&gt;One of the crucial steps before learning any Machine Learning model is data preprocessing. Most of the time we just center the data and set variance to 1 (as this make the optimalization process easier). In case of Face-Recognition, we need one-more step: Face-Alignment. What is this all about? In brief, we want to facilitate the task for out model by making the position of face constant (most of the time it means that eyes, nose and mouth are roughly at same position for every image). This is a pretty common step for all known Face-Recognition algorithm. Commonly to other posts, we will ask the beginning question:&lt;br&gt;
&lt;strong&gt;Which preprocessing method for Face-Recognition is best?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reading this post you will find:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Which Face-Alignment method is best? Should we even care about it?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How many pad add around the face before feeding to image?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Which Image transform should we use?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Does quality of Facial Landmark really care?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Does number and position of reference face landmark influence the accuracy?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It is possible to learn a model for Face-Alignment?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="facealignmentmethod"&gt;Face-Alignment method&lt;/h2&gt;
&lt;p&gt;First of all, let's clarify what we need for preprocessing (most of the time):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Detected face in image&lt;/li&gt;
&lt;li&gt;Face landmarks&lt;/li&gt;
&lt;li&gt;Reference landmarks points/pose of face&lt;/li&gt;
&lt;li&gt;Chosen type of transformation (ex. Similarity, Affine)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's assume that we have image with detected face using any provided algorithm (it does not influence the final accuracy of model, it may just make dataset bigger/smaller depending on number of detected faces in image, we use &lt;a href="https://arxiv.org/abs/1604.02878"&gt;MTCNN&lt;/a&gt;). Then, we must decide if we want to make use of 2D alignment (most popular one) or 3D.&lt;/p&gt;
&lt;h3 id="2dalignment"&gt;2D Alignment&lt;/h3&gt;
&lt;p&gt;At first, we will go step-by-step in 2D case. Firstly we need a face landmarks. There are many available algorithm for that task, which may return different number of such points, depending on training data annotations (there are even competiton for this task: &lt;a href="https://pdfs.semanticscholar.org/657a/58a220b1e69d14ef7a88be859d2f8d75e6a1.pdf"&gt;Menpo Benchmark&lt;/a&gt;). The most popular numbers are &lt;code&gt;68&lt;/code&gt; or &lt;code&gt;5&lt;/code&gt;. Why we may want to choose one set of points vs another? I all just depend on our reference points, which are the points of our base face position (so position of eyes,mouth etc.). We will try to make landmarks of each face as close to them as possible. How we can obtain/calculate such reference points? It is good question. We found a three ways of obtaining them:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;taking already known points, ex. from &lt;a href="https://github.com/wy1iu/sphereface/blob/master/preprocess/code/face_align_demo.m#L22"&gt;SphereFace&lt;/a&gt; paper&lt;/li&gt;
&lt;li&gt;find one base image with frontal face and take points from it&lt;/li&gt;
&lt;li&gt;calculate mean position of each point&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having reference points, next step is choosing the padding of of face, where we mean that there can be a tight or loosely crop of face (ex. containing hair, ears). Just look at example images and see that having the same reference point and different value of padding can result in different image. Also, you could have different value of padding in height and width direction (as most of faces are oval)&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/merge_padding.png"&gt;
&lt;/p&gt;
&lt;p&gt;After this step, we have the last one: choosing the way of calculating transformation which will make landmarks from test images the most similar to reference points. Here we have 4 possible ways of doing it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Euclidean Transformation&lt;/li&gt;
&lt;li&gt;Similarity Transformation&lt;/li&gt;
&lt;li&gt;Affine Transformation&lt;/li&gt;
&lt;li&gt;Projective Transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The very good lecture about this transformation is from &lt;a href="https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws11-12/3DCV_WS11-12_lec04.pdf"&gt;Augmented Vision Lab from University of Kaiserslautern&lt;/a&gt;. Let's also quickly clarify it here.&lt;/p&gt;
&lt;h4 id="euclideantransformation"&gt;Euclidean Transformation&lt;/h4&gt;
&lt;p&gt;It is rigid transformation which preserve distance between each pair of points. The Euclidean transformations include rotations and translations (3DoF).&lt;/p&gt;
&lt;h4 id="similaritytransformation"&gt;Similarity Transformation&lt;/h4&gt;
&lt;p&gt;In contrast to Euclidean, this transformation also include scaling, so it can make images smaller/bigger (so does not preserve distances between points, 4Dof)&lt;/p&gt;
&lt;h4 id="affinetransformation"&gt;Affine Transformation&lt;/h4&gt;
&lt;p&gt;Transformation which preserves points, straight lines and planes. Examples of affine transformations include translation, rotation, scaling, changing aspect ratio and shear mapping (6DoF).&lt;/p&gt;
&lt;h4 id="projectivetransformation"&gt;Projective Transformation&lt;/h4&gt;
&lt;p&gt;It is the most advanced transformation, which in contrast to Affine does not preserve parallelism of lines. In contrast to other transformation, it create vanishing points and horizonts. As we look into transformation matrix we can notice that each parameter is independent (8DoF).&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/transformation.png" height="450" width="450"&gt;
&lt;/p&gt;
&lt;p&gt;What is the visual difference between this transformation? Let's look into images. If we consider just MSE between reference points and projected points, the more degree of freedom we have then projection error is lower. In the other hand, the more degree of freedom we have, the more unnatural images look like. What is the more important, low MSE error or natural looking faces?&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/compare_transform-1.png"&gt;
&lt;/p&gt;    
&lt;h3 id="3dalignment"&gt;3D Alignment&lt;/h3&gt;
&lt;p&gt;So there are the steps for alignment of faces in 2D. 3D case in much less popular, but it was used one of the breakthourgh paper in Face Recognition, &lt;a href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/"&gt;Deep Face&lt;/a&gt;. How the basic pipeline look here? In fact it depend on the algorithm because each of them have different pipeline. We are not the specialist at 3D Alignment as most of case it does not work better that 2D, most of time because of higher level of interpolation (like piecewise-affine transformation). Below images represent different approaches to 3D transformation.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/deepface.png"&gt;
&lt;/p&gt;
&amp;nbsp
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/hasser.png"&gt;
&lt;/p&gt;
&lt;h2 id="experimentbasedon2dalignment"&gt;Experiment based on 2D Alignment&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Compared to earlier experiments, we would have different the &lt;strong&gt;Baseline&lt;/strong&gt; model, exactly we will replace linear layer after last module with &lt;a href="https://arxiv.org/abs/1406.4729"&gt;Spatial Pooling&lt;/a&gt; (which is invariant to input size, always output the same size of features). We propose such idea because in next experiments we will have different input size (96x112 and 112x112). We could just increase the number of parameters in model but this could cause also increased overfitting then experiments would not be comparable. Worth noting that this model achieve slightly higher results at &lt;code&gt;LFW&lt;/code&gt; but much lower at &lt;code&gt;BLUFR&lt;/code&gt; protocol.&lt;/p&gt;
&lt;p&gt;In first experiment we will compare multiple method of 2D alignment which differ with reference points and padding around the face. All method are open-source and used by Face-Recognition project which can be find at GitHub. Here are the tested methods:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Basic: using MTCNN for points and &lt;a href="https://github.com/wy1iu/sphereface/blob/master/preprocess/code/face_align_demo.m"&gt;SphereFace&lt;/a&gt; reference points. Use Similarity transformation.&lt;/li&gt;
&lt;li&gt;Crop: method taken from &lt;a href="https://github.com/davidsandberg/facenet"&gt;Facenet&lt;/a&gt; by David Sandberg, it just crop image with padding&lt;/li&gt;
&lt;li&gt;Dlib: using &lt;a href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html"&gt;dlib&lt;/a&gt; method for Face-Aligment (get_face_chips) with 112 image size and 25% of padding (default value). This method use image-pyramid to make downsampling higher quality and 5 points (but different than SphereFace). Use Affine transformation.&lt;/li&gt;
&lt;li&gt;OpenFace-3points:using original &lt;a href="https://github.com/cmusatyalab/openface"&gt;OpenFace&lt;/a&gt; method for Face Alignment, which use 3 point as a reference. Use Affine transformation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/compare_ref_points-2.png"&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/compare_org-1.png"&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/119_basic_exp.png" height="155" width="1140"&gt;
&lt;/p&gt; 
&lt;p&gt;Look like the SpherFace method is best (we think it is one of the reason of high performance of their algorithm). But surprisingly the second one is just raw face-crop without any alignment. It is pretty intriguing that is worth to mention that &lt;a href="https://arxiv.org/abs/1503.03832"&gt;FaceNet&lt;/a&gt; network from Google was also learn using raw images. Also recently there were several paper which claim that face-alignment is even harmful:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w6/papers/Ferrari_Investigating_Nuisance_Factors_CVPR_2017_paper.pdf"&gt;Investigating Nuisance Factors in Face Recognition with DCNN Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.04823"&gt;To Frontalize or Not To Frontalize: A Study of Face Pre-Processing Techniques and Their Impact on Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will look closer this statement, but compared to cited paper we will be comparing best Face-Alignment method vs Raw-Image.&lt;/p&gt;
&lt;p&gt;Continuing the analysis of basic Face-Alignment method, we are not sure why &lt;code&gt;Dlib&lt;/code&gt; method achieved so low results, maybe because of not including the upper part of head? The algorithm proposed in &lt;code&gt;OpenFace&lt;/code&gt; is the weakest, it just contain to less details of face (as of the the creator of &lt;code&gt;OpenFace&lt;/code&gt; code, we are really disappointed about that, in the other hand now we know what was one of the reason of poor performance).&lt;/p&gt;
&lt;p&gt;As we analyse some basic method of Face-Alignment, now let's make more in-depth analysis of each component.&lt;/p&gt;
&lt;h2 id="whichkindofimagetransformationshouldwechoose"&gt;Which kind of Image Transformation should we choose?&lt;/h2&gt;
&lt;p&gt;Let's make a quick experiment of comparing the image transformation. We will exclude the Euclidean transform as without scaling we should have scale invariant reference points and we don't. In this experiment we will be using baseline reference points (from &lt;code&gt;SphereFace&lt;/code&gt; project).&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/117_image_trans.png" height="123" width="1138"&gt;
&lt;/p&gt; 
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/11/compare_transform-1.png" height="300" width="300"&gt;
&lt;/p&gt;  
&lt;p&gt;Look like that more natural images are better than fitting the reference points better. Even adding changing aspect ratio and shear make error slightly worse.&lt;/p&gt;
&lt;h2 id="doesnumberofreferencepointsmatters"&gt;Does number of reference points matters?&lt;/h2&gt;
&lt;p&gt;As &lt;code&gt;OpenFace&lt;/code&gt; provide 68 reference landmarks, we will use their software to make such test and check if more mean better. Also maybe some location of points are better than other. For testing is we choose following cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 points: eyes + nose&lt;/li&gt;
&lt;li&gt;3 points: eyes + center of mouth&lt;/li&gt;
&lt;li&gt;7 points: eyes + nose + 2 points from mouth&lt;/li&gt;
&lt;li&gt;68 points: all points detected in face&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First two are defined at OpenFace. 7 points case was added as more complicated version, our imagination which points should be used (points are similar to references in SphereFace). The last case is just using all points. For each case we use Similarity Transformation.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/openface-points-1.png" height="400" width="400"&gt;
&lt;/p&gt;  
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/123_points_matters.png" height="156" width="1138"&gt;
&lt;/p&gt; 
&lt;p&gt;The results are not surprise for us, but maybe there are for you? So, look like 7 points and 68 points case works best, overall much better than basic setting for OpenFace. Why? Look like when choosing the reference points there should be suitable variety of points location, in the other hand the transformation would be not consistent/incorrect for many faces. Just look at results of &lt;code&gt;openface-3points-lip&lt;/code&gt;, which point have not usual location. In such case the transformation in calculated using only small region of image, what could cause calculating transformation which we are not wanting too. It's also interesting that having 7 or 68 points create similar results, we supposed that 68 should be worse as face-edge points are not always visible what should cause undefined behaviour. But look like Similarity transform works nice with both cases. However, we would choose 7-points case which should generalize better. In summary, the number of reference points may be not as crucial, even 7 (or 5 like in our basic method) works nice. The most important thing is using points which extend over a large face area.&lt;/p&gt;
&lt;h2 id="howaccuracyoffacelandmarklocationaffectthemodel"&gt;How accuracy of face landmark location affect the model?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;MTCNN&lt;/code&gt; algorithm was developed for Face-Detection and for Face-Landmark detection. As you may known, there also exist specialized method for just landmarks, which achieve higher accuracy. How does the more accurate landmark position affect final model? For this experiemnt we will use one of state-of-the-art models, &lt;a href="https://github.com/MarekKowalski/DeepAlignmentNetwork"&gt;DeepAlignmentNetwork&lt;/a&gt; created by &lt;a href="http://home.elka.pw.edu.pl/~mkowals6/doku.php"&gt;Marek Kowalski&lt;/a&gt;. The idea behind their algorithm is much more complicated than &lt;code&gt;MTCNN&lt;/code&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/kowalski.png" height="400" width="400"&gt;
&lt;/p&gt;  
&lt;blockquote&gt;
&lt;p&gt;DAN is a multi-stage convolutional neural network for face alignment, where each stage analyzes the entire face image. This is facilitated thanks to the use of landmark heatmaps and feature images which transfers the information about the landmarks between stages. The use of an entire face image makes DAN very robust, which leads to state of the art accuracy on the most difficult datasets, including 72% reduction of failure rate on the 300W public test set.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Also &lt;code&gt;DAN&lt;/code&gt; detect 68 points and unfortunately they don't overlap with &lt;code&gt;MTCNN&lt;/code&gt; points. To be able to use same reference points, we estimate the position of needed points (center of eyes, nose and center of month) using other detected points. The final points look good at pictures, but we must be aware of the possibility of a mistake already at the beginning of our experiment. Here are the results.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/124_kowalski.png" height="91" width="1138"&gt;
&lt;/p&gt; 
&lt;p&gt;Look like the &lt;code&gt;DAN&lt;/code&gt; algorithm does not influence of the final performance of model, the both result look really similar (and difference maybe not statistically significant, we could check it by doing several experiments, not once). Even our estimation of points does not hurt the performance. Maybe we don't need these points at all and using just raw image? Let's discuss it next.&lt;/p&gt;
&lt;h2 id="isreallyfacealignmentworthdoing"&gt;Is really Face-Alignment worth doing?&lt;/h2&gt;
&lt;p&gt;As stated in &lt;a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w6/papers/Ferrari_Investigating_Nuisance_Factors_CVPR_2017_paper.pdf"&gt;Investigating Nuisance Factors in Face Recognition with DCNN Representation&lt;/a&gt;, Face Recogniton may not need Face-Aligment at all. In this section I would like to repeat their experiments but using our network architecture and &lt;code&gt;LFW/BLUFR&lt;/code&gt; for testing. The intuition tells us that this situation seems impossible. If it would, why so much effort would was put in to find a good method for Face-Alignment.&lt;br&gt;
Our test will be very similar to presented is paper and  basic alignment method would be presented by &lt;code&gt;SphereFace&lt;/code&gt;. We will be testing different value of bounding-boxes around the face (parameterized by padding), like presented below. The size of input image from cropped faces is always 112x112. For aligned faces the longer side have value 112. Worth noting that mentioned paper used images 224x224.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/blog3-crop-pad.jpg" height="400" width="400"&gt;
&lt;/p&gt;  
&lt;p&gt;Here are results from both experiments.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/121_crop_pad.png" height="189" width="1138"&gt;
&lt;/p&gt; 
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/120_mtcnn_pad.png" height="183" width="1138"&gt;
&lt;/p&gt; 
&lt;p&gt;In contrast to cited paper, our experiments show the clear advantage of learning using faces after alignment in contrast to use raw faces. The different is &amp;gt;5%, so we think that Alignment is it worth doing.&lt;br&gt;
Some paper like &lt;a href="https://arxiv.org/abs/1503.03832"&gt;FaceNet&lt;/a&gt; or &lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"&gt;VGGFaceNet&lt;/a&gt; claim that when training with just cropped images, it is good idea to just align it for testing. But we are not sure how this image should look like as we do not have any reference points. We tried running &lt;code&gt;SphereFace&lt;/code&gt; and &lt;code&gt;Dlib&lt;/code&gt; alignment with different padding with no success. Our last try was just rotating the face so that eyes would be at one level without making any significant translation. However the results are not better than baseline. So we were not able to confirm that using aligned images for model learned on cropped faces boost the accuracy.&lt;br&gt;
However, for us this is kind of phenomena that cropping is working so well, but why? Our first intuition is that could be treated as Data Augmentation technique, because the faces are rotated relative to the center of the picture with different values. We will add &lt;code&gt;rotation&lt;/code&gt; to our sets of Data Augmentation technique, which will be tested in next post.&lt;/p&gt;
&lt;p&gt;We would like also discuss a additional phenomena: both 3 middle experiments  in their group achieve similar accuracy despite having different padding value. It just only show that network can distinguish between useful (foreground) and superfluous (background) information, what create some kind of attention, nice! Let's make the test by taking the feature map from one of convolutional module and visualize it. Our visualization will be not as nice as others because most of blogs/paper use last layer to show per class attention. We are just want to show the focus of entire  network before making any classification (so in fact where the features which describe the person come from). The displayed value is exactly mean absolute value. Let's hope they would look nice! Many test with different  configuration of padding will be taken. Enough talking, network, show what you have inside! (As you may notice, the more &lt;code&gt;pink&lt;/code&gt; the color is, the higher value of features is)&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/blog3_crop_Attention-1.png" height="600" width="600"&gt;
&lt;/p&gt; 
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/blog3_align_attention-1.png" height="600" width="600"&gt;
&lt;/p&gt; 
&lt;p&gt;Above images look intriguing, don't they? Look like some of network have higher values in face-region, other not. Also network learned with aligned faces have overall more &lt;code&gt;pinky&lt;/code&gt; images. But whether it adds anything to interpretation of network? Our answer is clear: besides it is nice, nothing special. As we are visualizing only just intermediate features, this state where network focus their attention. But you must be aware that this values are not normalized, so visualization show only peak values, in fact the attention map could be much bigger. Also why the &lt;code&gt;Attention Map&lt;/code&gt; from some network with very high score doesn't look great? If you want to get better intuition about &lt;code&gt;visualizing the feature maps&lt;/code&gt; we advice lecture from &lt;a href="http://course.fast.ai/"&gt;fast.ai&lt;/a&gt;  part 1: &lt;a href="http://course.fast.ai/lessons/lesson7.html"&gt;7—EXOTIC CNN ARCHITECTURES; RNN FROM SCRATCH&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="howaboutlettingthenetworktoaligntheimage"&gt;How about letting the network to align the image?&lt;/h2&gt;
&lt;p&gt;As we let the network to learn which features are crucial for Face-Recognition, maybe we also let it to preprocess the images? Sound great, but how come? Just use &lt;a href="https://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Layer&lt;/a&gt;! It enable to apply the image transformation based on features extracted from image, so no need for face landmarks.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="https://raw.githubusercontent.com/moodstocks/gtsrb.torch/master/resources/spatial-transformer-structure.png" height="600" width="600"&gt;
&lt;/p&gt; 
&lt;p&gt;As presented at above images, it contain:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Localication network: used for generating 6 parameters needed for image transformation&lt;/li&gt;
&lt;li&gt;Grid generator: use provided 6 parameters to generate transformation grid&lt;/li&gt;
&lt;li&gt;Sampler: used input image and grid output modified image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you would like to get more intuition about &lt;code&gt;STN&lt;/code&gt;, read a introduction blog about it from &lt;a href="https://kevinzakka.github.io/2017/01/10/stn-part1/"&gt;kevinzakk&lt;/a&gt;, we really recommed it!.&lt;/p&gt;
&lt;p&gt;This approach is not novel at Face-Recognition community, we know 3 papers already use this approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.07270"&gt;Learning a Metric Embedding for Face Recognition using the Multibatch Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1701.07174"&gt;Towards End-to-End Face Recognition through Alignment Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.10818"&gt;End-To-End Face Detection and Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of mentioned paper use &lt;code&gt;STN&lt;/code&gt; in different way. The first paper use similarity transform and pretrain the model using landmark annotationthen it is finetuned with recognition part. The second one want to compare different type of transformation (like we did earlier in the post) but using learnable &lt;code&gt;STN&lt;/code&gt;. The last one use &lt;code&gt;STN&lt;/code&gt; also face-detector.&lt;/p&gt;
&lt;p&gt;Our approach would be the most similar to the paper &lt;a href="https://arxiv.org/abs/1701.07174"&gt;Towards End-to-End Face Recognition through Alignment Learning&lt;/a&gt; (despite using &lt;a href="http://ydwen.github.io/papers/WenECCV16.pdf"&gt;CenterLoss&lt;/a&gt;), but we will also try pretraining &lt;code&gt;STN&lt;/code&gt; model.&lt;br&gt;
Out input data is cropped images with some of the paddding with final size &lt;code&gt;150x150&lt;/code&gt;. The network architecture is 3 layer Convolutional with Pooling as a first component (which behave as a downsampler here). The input size to network is &lt;code&gt;112x96&lt;/code&gt; (like in baseline experiment). This is all we need here, let's make experiments!&lt;/p&gt;
&lt;h4 id="pretrainedstn"&gt;Pretrained STN&lt;/h4&gt;
&lt;p&gt;First experiments will just show if &lt;code&gt;STN&lt;/code&gt; are able to learn transformation similar to our &lt;code&gt;baseline&lt;/code&gt; and if we could easily boost accuracy by finetunning it.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/122_st_finetine.png" height="157" width="1138"&gt;
&lt;/p&gt; 
&lt;p&gt;From all &lt;code&gt;pretraining&lt;/code&gt; experiments the best results achieved method, where &lt;code&gt;STN&lt;/code&gt; is pretrained  and then frozen during training connected with the &lt;code&gt;baseline FaceResNet&lt;/code&gt; model. The results as against &lt;code&gt;baseline&lt;/code&gt; are comparable beyond the &lt;code&gt;validation loss&lt;/code&gt;  and &lt;code&gt;validation accuracy&lt;/code&gt;, where STN model archive much higher values. In other cases when &lt;code&gt;STN&lt;/code&gt; model is finetuned or even &lt;code&gt;FaceResNet&lt;/code&gt; is learned from scratch, the results are much lower than the &lt;code&gt;baseline&lt;/code&gt;. Concluding this part, it is not worth to use pretrained &lt;code&gt;STN&lt;/code&gt; model in any configuration as final result in best case are same like the &lt;code&gt;baseline&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="typesoftransformationinstn"&gt;Types of transformation in STN&lt;/h4&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/125_st_tranforamation.png" height="122" width="1138"&gt;
&lt;/p&gt; 
&lt;p&gt;These experiments were testing basic type of transformation which can be learned using &lt;code&gt;STN&lt;/code&gt;. What are the results? The best method (&lt;code&gt;affine transformation&lt;/code&gt;) does achieve similar performance like the best current method taken from &lt;code&gt;SpherFace&lt;/code&gt; paper. We were really surprised by this results! This indicate that &lt;code&gt;STN&lt;/code&gt; really can learn useful transformation. Let's also look at how this image transformation look like, how different they are compared to &lt;code&gt;baseline&lt;/code&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/blog3-trans-stn.jpg" height="300" width="300"&gt;
&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;STN&lt;/code&gt; transformation are similar to each other and different from presented in &lt;code&gt;SphereFace&lt;/code&gt; paper. This indicate that there are many ways of Face-Alignment to get high accuracy of model.&lt;/p&gt;
&lt;h4 id="morestnexperiments"&gt;More STN experiments&lt;/h4&gt;
&lt;p&gt;Intrigued by high performance of &lt;code&gt;STN&lt;/code&gt; we conduct some more experiments. We were adding more parameters  or &lt;code&gt;DropOut&lt;/code&gt; to &lt;code&gt;Localization Network&lt;/code&gt;. Also original &lt;code&gt;baseline FaceResNet&lt;/code&gt; (without &lt;code&gt;AvgPool&lt;/code&gt; model was tested).&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/126_st_more_exp.png" height="360" width="1138"&gt;
&lt;/p&gt; 
&lt;p&gt;It look like that adding more parameters to model make &lt;code&gt;STN&lt;/code&gt; better in context of &lt;code&gt;Validation Loss&lt;/code&gt;. However &lt;code&gt;LFW&lt;/code&gt; and &lt;code&gt;BLUFR&lt;/code&gt; are going in opposite direction because the results are getting worse. Also the original &lt;code&gt;baseline FaceResNet&lt;/code&gt; model does not get any boost of performance in &lt;code&gt;LFW&lt;/code&gt; and &lt;code&gt;BLUFR&lt;/code&gt; despite having the competitive &lt;code&gt;Validation Loss&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In summary, look like &lt;code&gt;STN&lt;/code&gt; can provide very competitive results, especially with data distribution seen during training. Notwithstanding it is not clear why the same improvement is not seen at &lt;code&gt;LFW&lt;/code&gt; and &lt;code&gt;BLUFR&lt;/code&gt; benchmarks (for example models with same &lt;code&gt;Validation Loss&lt;/code&gt; like &lt;code&gt;STN&lt;/code&gt; model get much better results). Our intuition say that it may be caused by overfitting &lt;code&gt;STN&lt;/code&gt; model to some &lt;code&gt;face distribution&lt;/code&gt; then applying same transform for different distribution does not provide as huge boost. If this would be true, when more model regularization method should be used (&lt;a href="https://arxiv.org/abs/1701.07174"&gt;Towards End-to-End Face Recognition through Alignment Learning&lt;/a&gt; use &lt;code&gt;CenterLoss&lt;/code&gt; as a additional regularization, maybe this is a clue for a better performance?). We will for sure get back to this method.&lt;/p&gt;
&lt;h2 id="3dfacealignment"&gt;3D Face Alignment&lt;/h2&gt;
&lt;p&gt;In the last experiments we would like to conduct experiments with the most advanced technique for Face Alignment, 3D models. However, the &lt;code&gt;3D&lt;/code&gt; pipeline is much more complicated with different training and testing methodology. Authors of most advanced method &lt;a href="https://arxiv.org/abs/1708.07517"&gt;FacePoseNet: Making a Case for Landmark-Free Face Alignment&lt;/a&gt;  were able to dispel our doubts about the pipeline the state-of-the-art method of 3D face modeling (see the comments). We need to read all the three paper carefully then maybe we will be able to create the &lt;code&gt;FacePoseNet&lt;/code&gt; pipeline.&lt;/p&gt;
&lt;p&gt;However, we run just experiments with different, much simpler method of 3D modeling, descibed in &lt;a href="https://www.openu.ac.il/home/hassner/projects/frontalize/"&gt;Effective Face Frontalization in Unconstrained Images&lt;/a&gt;. The results does not convince us to make any further efforts in this direction as many of images doesn't work well (especially with rotated faces).&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition IV: Face-Alignment" src="http://localhost:2368/content/images/2017/12/118_3d.png" height="65" width="1138"&gt;
&lt;/p&gt; 
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It this post we have analysed a lot of ways of preparing &lt;code&gt;Face&lt;/code&gt; image before using any Deep Learning model. Unfortunately none of method was able to detronize method proposed in &lt;code&gt;SphereFace&lt;/code&gt;. Let's first answer our question asked at the beginning:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Which Face-Alignment method is best? Should we even care about it?&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;the best method is proposed by creators of &lt;code&gt;SphereFace&lt;/code&gt; (&lt;a href="https://github.com/wy1iu/sphereface/blob/master/preprocess/code/face_align_demo.m"&gt;code example&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How many pad add around the face before feeding to image?&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;in case of &lt;code&gt;SphereFace&lt;/code&gt; method, between 0 and 20&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Which Image transform should we use?&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Similarity Transformation&lt;/code&gt; provide best results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Does quality of Facial Landmark really care?&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;when you have only use &lt;code&gt;5&lt;/code&gt; of them, it does not significantly influence the final results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Does number and position of reference face landmark influence the accuracy?&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;choosing the both number and position of reference points is crucial for model accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It is possible to learn a model for Face-Alignment?&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;STN&lt;/code&gt; show that it is possible with results only worse that &lt;code&gt;SphereFace&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are also some extra positives sides of experiments as we can now give some recommendation for others researcher:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When you are trying to create own &lt;code&gt;Face Alignment&lt;/code&gt; algorithm always make sure that is surpass single image cropping (which is surprisingly hard baseline to beat).&lt;/li&gt;
&lt;li&gt;The position of reference &lt;code&gt;Face Landmark&lt;/code&gt; and amount of &lt;code&gt;padding&lt;/code&gt; around the face is crucial for &lt;code&gt;Face Alignment&lt;/code&gt; task.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is all in this post, we hope that there were interesting experiments for you. Next time we will test a &lt;code&gt;Data Augmentation&lt;/code&gt; technique and their relation to &lt;code&gt;Face Recognition&lt;/code&gt; task.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;h3 id="papers"&gt;Papers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1704.08063.pdf"&gt;SphereFace : Deep Hypersphere Embedding for Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.02878"&gt;Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pdfs.semanticscholar.org/657a/58a220b1e69d14ef7a88be859d2f8d75e6a1.pdf"&gt;Menpo Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/"&gt;DeepFace: Closing the Gap to Human-Level Performance in Face Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1503.03832"&gt;FaceNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1406.4729"&gt;Spatial Pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w6/papers/Ferrari_Investigating_Nuisance_Factors_CVPR_2017_paper.pdf"&gt;Investigating Nuisance Factors in Face Recognition with DCNN Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1610.04823"&gt;To Frontalize or Not To Frontalize: A Study of Face Pre-Processing Techniques and Their Impact on Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1706.01789"&gt;Deep Alignment Network: A convolutional neural network for robust face alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"&gt;Deep Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1605.07270"&gt;Learning a Metric Embedding for Face Recognition using the Multibatch Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1701.07174"&gt;Towards End-to-End Face Recognition through Alignment Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.10818"&gt;End-To-End Face Detection and Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1708.07517"&gt;FacePoseNet: Making a Case for Landmark-Free Face Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.openu.ac.il/home/hassner/projects/frontalize/"&gt;Effective Face Frontalization in Unconstrained Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="projectsblogstutorials"&gt;Projects, blogs, tutorials&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/davidsandberg/facenet"&gt;Facenet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html"&gt;dlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cmusatyalab/openface"&gt;OpenFace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://course.fast.ai/"&gt;fast.ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kevinzakka.github.io/2017/01/10/stn-part1/"&gt;Deep Learning Paper Implementations: Spatial Transformer Networks - Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ags.cs.uni-kl.de/fileadmin/inf_ags/3dcv-ws11-12/3DCV_WS11-12_lec04.pdf"&gt;Augmented Vision Lab from University of Kaiserslautern&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition III: Noise</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;In last years there were introduced many dataset for Face Recognition, named several:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt;: 0.5M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vintage.winklerbros.net/facescrub.html"&gt;FaceScrub&lt;/a&gt;: 0.1M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.msceleb.org/celeb1m/dataset"&gt;MsCeleb&lt;/a&gt;: 10M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.umdfaces.io"&gt;UMD&lt;/a&gt;: 0.38M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/"&gt;VGG&lt;/a&gt;: 2.6M of images&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main difference between them are the number of images and identities. But&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/2017/11/09/demystifying-face-recognition-iii-noise/</link><guid isPermaLink="false">5a047a1a1eb5111c6fa985dc</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Thu, 09 Nov 2017 16:15:35 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/logo-1.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2017/11/logo-1.png" alt="Demystifying Face Recognition III: Noise"&gt;&lt;p&gt;In last years there were introduced many dataset for Face Recognition, named several:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt;: 0.5M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vintage.winklerbros.net/facescrub.html"&gt;FaceScrub&lt;/a&gt;: 0.1M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.msceleb.org/celeb1m/dataset"&gt;MsCeleb&lt;/a&gt;: 10M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.umdfaces.io"&gt;UMD&lt;/a&gt;: 0.38M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/"&gt;VGG&lt;/a&gt;: 2.6M of images&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main difference between them are the number of images and identities. But they're also different in sth like &lt;code&gt;signal-to-noise ratio&lt;/code&gt;, where we mean that ratio between correctly labeled images to all collected images. We do not know such value for any provided database, but some researcher provided the clean-list which provided images which was verified to be correct (by human or face-recognition algorithm). For example &lt;strong&gt;CASIA-WebFace&lt;/strong&gt; have 90% of correct images, also &lt;strong&gt;VGG&lt;/strong&gt; have ~90% (supposing that 15% of noisy labels is a real noise). But what does it mean relative to accuracy of Face Recognition model? Does noise really hurt the performance in verification/identification protocol?&lt;/p&gt;
&lt;h2 id="noisydatabase"&gt;Noisy Database&lt;/h2&gt;
&lt;p&gt;First of all, let's make some literature review.&lt;br&gt;
One of the publication which we know about noise in data for Deep Learning is &lt;a href="https://arxiv.org/abs/1511.06789"&gt;The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition&lt;/a&gt;. The idea was simple: instead of marking pictures manually use information gathered in Web (metadata and search-engine). This enable to collect many more images than existed in current training dataset of several benchmarks. The results indicate that when training on original images and from web, the final accuracy is always higher (despite the fact, that the data in noisy).&lt;/p&gt;
&lt;p&gt;The first public face-recognition database with significant amount of noise was &lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"&gt;VGG&lt;/a&gt;. We will describe their method in more details. They collected 5M images from both Google and Bing Image Search. Then using Machine Learning algorithm with Fisher Vector Faces descriptor, half of images was removed. The last stage was manually filtering, which take 10 days (really hard work), ending with ~1M of images. Additional the images before manually filtering were provided, what end up with 2.6M. In experiments, Oxford team was using both dataset (named full and curated). And the bigger one leads to better results at all of their experiments, although it contain more noisy images than right one. Before jumping to any conclusion, let's take a closer look into manually filtering process by reading the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The aim of this final stage is to increase the purity (precision) of the data using human annotations. However, in order to make the annotation task less burdensome and hence avoid high annotation costs, annotators are aided by using automatic ranking once more. This time, however, a multi-way CNN is trained to discriminate between the 2,622 face identities using the AlexNet architecture; then the softmax scores are used to rank images within each identity set by decreasing likelihood of being an inlier. In order to accelerate the work of the annotators, the ranked images of each identity are displayed in blocks of 200 and annotators are asked to validate blocks as a whole. In particular, a block is declared good if approximate purity is greater than 95%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So filtering was done using block of images, which were accepted or refused by looking into purity of it. It means that if there is even some noise in block (more than 5%), then block is removed. To sum up, in 1.6M of noisy images there could be even ~1.4 of good labels. The similar situation is at other dataset like &lt;strong&gt;CASIA&lt;/strong&gt; or &lt;strong&gt;MsCeleb&lt;/strong&gt;: we are not sure that noise is really a noise, because most of time manually annotators does not investigate each of image independently. But from both of them researchers provided list of clean images by removing the noise manually (CASIA) or in automatic way (MsCeleb).&lt;/p&gt;
&lt;h2 id="experimentsoncasiawebface"&gt;Experiments on CASIA-WebFace&lt;/h2&gt;
&lt;p&gt;The first test will be based on &lt;strong&gt;UMD&lt;/strong&gt; database (as very clean database) and different version of &lt;strong&gt;CASIA-WebFace&lt;/strong&gt; database:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;full (0.5M of images)&lt;/li&gt;
&lt;li&gt;clean-v1 (0.47M, taken from &lt;a href="https://github.com/happynear/FaceVerification"&gt;Happy-Near page&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;clean-v2 (0.445M, taken from project &lt;a href="http://zhengyingbin.cc/ActiveAnnotationLearning"&gt;Face Recognition via Active Annotation and Learning&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idea behind using this databases is testing algorithm on very clean database with smaller size (UMD) versus bigger and noisier database with some efforts of cleaning it. It is really worth to clean the dataset, which cost many of hard manual work?&lt;/p&gt;
&lt;p&gt;For training the network, we will be using the same setting like for baseline. Here are results.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="620" height="300" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/108.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;There is no clean conclusion from this experiments. First of all, we can see that delivered clean-list for CASIA-WebFace really have lower noise, because network trained using them have lower loss and higher validation accuracy. But this is the main difference, all network achieve similar score at LFW and BLUFR benchmarks. The highest score achieve the cleanest one, &lt;strong&gt;clean-v2&lt;/strong&gt;, but the difference is only ~1%. Maybe if dataset would contain more images, the bigger change would be noticed. &lt;strong&gt;UMD&lt;/strong&gt; database despite of being very clean, achieve the lowest score. From this set of experiment lool like that there is no big advantage of cleaning dataset if we suppose than there is no more than 10% of incorrect labels.&lt;/p&gt;
&lt;h2 id="experimentsonvggdatabase"&gt;Experiments on VGG-Database&lt;/h2&gt;
&lt;p&gt;As the first experiments does not make any clean conclusion, let's use &lt;strong&gt;VGG&lt;/strong&gt; database. As the number of noisy labels in VGG is much bigger, here will be conducted  experiments using different ratio of noisy labels (exactly images labeled as noisy to all images in training set). We hope that it will show how different amount of noisy labels impact the final accuracy (remembering that the noisy labels in case of VGG maybe may have 10-15% of real noise). For coherence of results, in each experiment same validation dataset is used.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="750" height="350" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/112.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;The conclusion from above experiment are the same like in original &lt;a href="http://localhost:2368/2017/11/09/demystifying-face-recognition-iii-noise/(http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf)"&gt;VGGFace&lt;/a&gt; paper: A  large dataset set is more important than a low level of noisy labels. However we do not know a real amount of noise in &lt;strong&gt;VGG&lt;/strong&gt; dataset. Let's check this in control condition. We will be using always the dataset of same size (0.75M of images), but with different % of real noise in labels (so this is the size situation when we have smaller training set + noisy data). We hope that this experiment will enable us to set a range of noise which is not harmful for final model accuracy.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="750" height="420" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/115.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;First of all, looks like to get meaningful result, the kFold idea should be used. Using only just one model make model more random (maybe it is connected with % of noise in data?). From current result we can see that having even 15% of noise make just slight lower score than pure one. Based on the VGG paper, it is more than they estimate at their dataset.&lt;/p&gt;
&lt;h2 id="experimentsusingrealnoise"&gt;Experiments using Real Noise&lt;/h2&gt;
&lt;p&gt;We decided to carry out last experiment. Using the cleanest version of CASIA-WebFace we will be injecting real noise to data with much more noise than before. The noise will be formed from images taken from VGG database with random labels. As we would like to have a small amount of noise in base training data, we will use &lt;strong&gt;clean-v2&lt;/strong&gt; list. The idea behind it is making the dataset bigger with just pure noise, how this effect the final accuracy?&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="750" height="420" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/114.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;Finally, this experiments provide us clear conclusion. The real noise, even in small amout, hurt the final accuracy. Although the model with 10% of noise achieve better score at LFW, the BLUFR protocol show the bigger difference (but both model are comparable). So we can conclude that is nice to have less than 10% of noise. Also strange situation happened with noise ratio of 50%: the network diverge, so we should lower the learning-rate.&lt;/p&gt;
&lt;p&gt;In the other hard, it is really intriguing that having the 80% of noisy labels still network can learn anything useful. The similar result was achieved by researcher in paper &lt;a href="https://arxiv.org/abs/1705.10694"&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;, even that the experiments have 99% of noise. The researcher have very novel conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep neural networks are able to learn from data that has been diluted by an arbitrary amount of noise.&lt;/li&gt;
&lt;li&gt;A sufficiently large training set is more important than a lower level of noise.&lt;/li&gt;
&lt;li&gt;Choosing good hyperparameters can allow conventional neural networks to operate in the regime of very high label noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;For sure Deep Learning is amazing in handling the noisy data, but we cannot agree with second conclusion, already owning ~0.5M of data example. Having 0.4M of images with clean labels is better than any database with larger amount of images but with noise. Other pros of having clean and small database is faster convergence (as there is less training examples). In the other hand, when we are collecting our own dataset it is inevitable to have noisy labels. Our experiments show that data should not have more than 10% of real noise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The summary of above experiments are harder than we except. Look like there is no clean conclusion from them because impact of noise depend on many variables (ex. data size, number of identities). We should make many more experiments with controlled environment to really analyse the impact of noisy labels to final accuracy of model.  but Here are our thoughts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the training dataset should not have more than 10% of real noise in dataset&lt;/li&gt;
&lt;li&gt;when we are able to gather easily dataset with noise less than 10%, it is worth doing it (maybe even this is better option rather than cleaning the dataset)&lt;/li&gt;
&lt;li&gt;making a dataset bigger by just inputting random images with random labels is not good idea&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ok, so test another aspect of preparing the dataset for Face-Recognition. The next question which will we ask about pipeline is:&lt;br&gt;
&lt;strong&gt;How the input image should be aligned before feeding it to network?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Learning Face Representation from Scratch”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vintage.winklerbros.net/Publications/icip2014a.pdf"&gt;A data-driven approach to cleaning large face datasets.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1607.08221"&gt;MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.01484v2"&gt;UMDFaces: An Annotated Face Dataset for Training Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"&gt;VGG-Deep Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1511.06789"&gt;The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1705.10694"&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zhengyingbin.cc/ActiveAnnotationLearning"&gt;Face Recognition via Active Annotation and Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/happynear/FaceVerification"&gt;Happy-Near page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition II: Baseline</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="testofdifferentnetworkarchitectures"&gt;Test of different network architectures&lt;/h2&gt;
&lt;p&gt;According to assumptions, the database is chosen (CASIA-WebFace), input image is preprocessed (112x96, MTCNN), only mirror used as Data Augmentation and for learning the CrossEntropy loss will be used. The last lacking element in the pipeline is network architecture. Last years was really abounded in&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/2017/11/08/demystifying-face-recognition-ii-baseline/</link><guid isPermaLink="false">5a0357fdd370791075dc45cc</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Wed, 08 Nov 2017 19:16:59 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/faceresnet.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="testofdifferentnetworkarchitectures"&gt;Test of different network architectures&lt;/h2&gt;
&lt;img src="http://localhost:2368/content/images/2017/11/faceresnet.png" alt="Demystifying Face Recognition II: Baseline"&gt;&lt;p&gt;According to assumptions, the database is chosen (CASIA-WebFace), input image is preprocessed (112x96, MTCNN), only mirror used as Data Augmentation and for learning the CrossEntropy loss will be used. The last lacking element in the pipeline is network architecture. Last years was really abounded in many diverse ideas about creating the architectures like &lt;strong&gt;ResNet&lt;/strong&gt;, &lt;strong&gt;Inception&lt;/strong&gt; or &lt;strong&gt;DenseNet&lt;/strong&gt;. Additional, the community of Face Recognition was also introducing their own architectures like &lt;strong&gt;FaceResNet&lt;/strong&gt;, &lt;strong&gt;SphereNet&lt;/strong&gt;, &lt;strong&gt;LightCNN&lt;/strong&gt; or &lt;strong&gt;FudanNet&lt;/strong&gt;. Currently we will look closer to the later one as we know their performance and low computation requirements.&lt;br&gt;
We will also include some older architectures to see if it is really true then the new ones works much better than architectures from 2014 or earlier, we choose &lt;strong&gt;LeNet&lt;/strong&gt;, &lt;strong&gt;DeepID&lt;/strong&gt;,** DeepID2+** and &lt;strong&gt;CASIA&lt;/strong&gt;.&lt;br&gt;
This is not the final choice of the architecture, we just want to get a reasonable baseline, which will accompany us with all others test. There are so many test, because we want to make sure, that my current pipeline works well and if my implementation match results from papers.&lt;/p&gt;
&lt;h2 id="descriptionofarchitectures"&gt;Description of Architectures&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;LeNet&lt;/a&gt; - the most popular convolutional architecture. Input image: 28x24.&lt;br&gt;
​&lt;br&gt;
&lt;a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf"&gt;DeepID&lt;/a&gt; - One of the first specialized networks used for Face Recognition. Comparing to LeNet, it have more filters and final feature comes from merging data from two layers. Input image: 42x36.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1412.1265"&gt;DeepID2+&lt;/a&gt; - Extension of &lt;strong&gt;DeepID&lt;/strong&gt;, have much more number of filters and features size is now 512. Input image:56x48.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Casia-Net&lt;/a&gt; - Architecture proposed after success of &lt;strong&gt;VGG&lt;/strong&gt; and &lt;strong&gt;GoogLeNet&lt;/strong&gt;. It use concept of kernel 3x3 and Average Pooling.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1511.02683"&gt;Light-CNN&lt;/a&gt;- The author propose to use &lt;code&gt;MFM&lt;/code&gt;as a activation function, which is extension of &lt;code&gt;MaxOut&lt;/code&gt;. In his experiments it is better than &lt;code&gt;ReLU&lt;/code&gt;, &lt;code&gt;ELU&lt;/code&gt; or even &lt;code&gt;PReLU&lt;/code&gt;.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1611.08976"&gt;FaceResNet&lt;/a&gt; - Architecture proposed by author of CenterLoss and RangeLoss, which use residual connection, like in ResNet. But it does not use BatchNorm and replaces the &lt;code&gt;Relu&lt;/code&gt; activation functions with the &lt;code&gt;PRELu&lt;/code&gt; functions.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1704.08063"&gt;SphereFace&lt;/a&gt; - New version of &lt;strong&gt;FaceResNet&lt;/strong&gt; which mainly replace each &lt;code&gt;MaxPool&lt;/code&gt; by &lt;code&gt;Convolution&lt;/code&gt; with stride equal to 2.&lt;br&gt;
​&lt;br&gt;
&lt;a href="http://www.yugangjiang.info/publication/icmr17-face.pdf"&gt;Fudan-Arch&lt;/a&gt; -Idea of &lt;strong&gt;FaceResNet&lt;/strong&gt; but with &lt;code&gt;Batchnorm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Most of the above architecture have also &lt;a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"&gt;DropOut&lt;/a&gt; inside, other have own regularization method. If we just want to replicate the results as stated at paper, we would still not be able to compare such results to each other because of different settings. This is why we will completely ignore any special regularization method (like &lt;a href="https://ydwen.github.io/papers/WenECCV16.pdf"&gt;CenterLoss&lt;/a&gt;) and here will be two experiments for each architecture: with and without DropOut. This would also help to validate the current implementation with the results from papers.&lt;br&gt;
To evaluate each network we will use its qualitative results (accuracy and loss value) and time-to-score. The model of each architect was chosen based on the model with the lowest validation loss, ie the result of the LFW did not affect the choice of the model, even though the models achieved better results in another epoch.&lt;/p&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p align="center"&gt;&lt;b&gt;CASIA Training and LFW&lt;/b&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="840" height="1020" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/92.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;iframe width="800" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/98.embed"&gt;&lt;/iframe&gt;
&lt;iframe width="800" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/97.embed"&gt;&lt;/iframe&gt;
&lt;p&gt;First of all, let's look closer to architectures with DropOut. There is not clean winner here, &lt;strong&gt;Fudan-Full&lt;/strong&gt;, &lt;strong&gt;SphereFace64&lt;/strong&gt; and &lt;strong&gt;Light-CNN29&lt;/strong&gt; are overall comparable, but each of them dominate at one of the given benchmark (validation loss, LFW, BLUFR). Very close to them in &lt;strong&gt;FaceResNet&lt;/strong&gt;, which was training much faster. It is very interesting that many network achieve &amp;gt; 97% at LFW, however BLUFR protocols show us the real difference in quality. For example, difference in 0.7% at LFW between &lt;strong&gt;CASIA&lt;/strong&gt; and &lt;strong&gt;SphereFace64&lt;/strong&gt; translate to 16% in BLUFR-FAR 1%.&lt;br&gt;
What about architectures without any regularization? Here the clear winner is &lt;strong&gt;Fudan-Full&lt;/strong&gt;, followed by &lt;strong&gt;SphereFace64&lt;/strong&gt; and &lt;strong&gt;FaceResNet&lt;/strong&gt;. From the intuition, it look like that &lt;code&gt;BatchNorm&lt;/code&gt; at &lt;strong&gt;Fudan-Ful&lt;/strong&gt;l helped at lot as it behaves like a regularizator. From such comparison we can also deduce which architecture is good for testing any new Data Augmentation technique or new loss, because it would show us even small gain. In our case it is &lt;strong&gt;Light-CNN29&lt;/strong&gt; which overfit a lot.&lt;/p&gt;
&lt;p&gt;For detailed analyse we choose best models: &lt;strong&gt;FaceResNet&lt;/strong&gt;, &lt;strong&gt;Light-CNN29&lt;/strong&gt;, &lt;strong&gt;SphereFace64&lt;/strong&gt; and &lt;strong&gt;FudanFull&lt;/strong&gt;.&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;IJB-A&lt;/b&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="800" height="260" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/106.embed"&gt;&lt;/iframe&gt;
 &lt;/p&gt;
&lt;p&gt;Overall here &lt;strong&gt;Light-CNN29&lt;/strong&gt; is the winner, which lose at only Rank-1 benchmark. But &lt;strong&gt;SphereFace64&lt;/strong&gt; is breathing down its neck by being just slightly worse. The results from &lt;strong&gt;FundanFull&lt;/strong&gt; are really bad, not sure what is the reason for that.&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;Mega Face&lt;/b&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="500" height="200" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/104.embed" align="center"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;In MageFace, identification protocol is winner by &lt;strong&gt;FudanFull&lt;/strong&gt; while verification protocol is taken by &lt;strong&gt;Light-CNN29&lt;/strong&gt; (where &lt;strong&gt;FudanFull&lt;/strong&gt; is again the weakest)&lt;/p&gt;
&lt;h2 id="baselinemodel"&gt;Baseline Model&lt;/h2&gt;
&lt;p&gt;Summarizing, if we want to choose best architecture among tested, the &lt;strong&gt;Light-CNN29&lt;/strong&gt; would be the best with &lt;strong&gt;Sphere64&lt;/strong&gt; just right behind. &lt;strong&gt;FudanFull&lt;/strong&gt; works nice, but in some scenario its accuracy is too low. This is our podium. Looking closer to the this architectures, the common thing is using residual connection. But they vary at activation function, using &lt;code&gt;Pool&lt;/code&gt; vs &lt;code&gt;Convolution&lt;/code&gt; with stride equal 2 and using &lt;code&gt;BatchNorm&lt;/code&gt;. So maybe they are not best possible architectures? We will leave this question for future tests.&lt;/p&gt;
&lt;p&gt;When we compare the results from current implementation with the results from paper, most of them matched target accuracy. The only exception is &lt;strong&gt;SphereFace&lt;/strong&gt;, which without DropOut overfit, although the original version does not have it.&lt;/p&gt;
&lt;p&gt;When we compare the time needed for getting best results, this is definitely the best place for FaceResNet, which is only slightly weaker than the best model, but it learned almost 3x shorter. This is why &lt;strong&gt;FaceResNet&lt;/strong&gt; is chosen as baseline architecture. He will accompany us throughout the series named &lt;strong&gt;Face Recognition&lt;/strong&gt;. Specifically, both &lt;strong&gt;FaceResNet&lt;/strong&gt; will be used, depending of scenario: when we will be reducing overfiting by new technique, we will use raw architecture, in other case DropOut will be used.&lt;/p&gt;
&lt;h2 id="whatnext"&gt;What next?&lt;/h2&gt;
&lt;p&gt;Looking into the results it look like that getting &lt;strong&gt;~98&lt;/strong&gt;% on LFW using only basic technique for learning is easy. This results would be among the best 3 years ago, but currently it is ~1.5% behind state-of-the-art. In MegaFace in even worse, because our results is &lt;strong&gt;20%&lt;/strong&gt; lower using same dataset.&lt;br&gt;
How we can boost accuracy of our model? A lot of researcher propose their own technique, but will they work in our case? What boost can we gain? We will learn this in the next post, and in the near future we will look at the aspect of noise in the learning data.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;Gradient-Based Learning Applied to Document Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf"&gt;Deep Learning Face Representation from Predicting 10,000 Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1412.1265"&gt;Deeply learned face representations are sparse, selective, and robust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Learning Face Representation from Scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ydwen.github.io/papers/WenECCV16.pdf"&gt;A Discriminative Feature Learning Approach for Deep Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.08976"&gt;Range Loss for Deep Face Recognition with Long-tail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1704.08063"&gt;SphereFace: Deep Hypersphere Embedding for Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.yugangjiang.info/publication/icmr17-face.pdf"&gt;Multi-task Deep Neural Network for Joint Face Recognition and Facial Aribute Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition I: Introduction</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;In Web can be find many articles and research paper about Face Recognition (just look at &lt;a href="https://scirate.com/search?utf8=%E2%9C%93&amp;amp;q=Face+recognition"&gt;scirate&lt;/a&gt;) . Most of them introduce or explain some new technique and compare it to baseline, without going into details about choosing elements of pipeline. There also exist some Open-Source, ready to use library for&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/2017/11/07/demystifying-face-recognition-i-introduction/</link><guid isPermaLink="false">5a02154c9bbba6631c618c5f</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Tue, 07 Nov 2017 20:20:59 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/Face-Recognition-Pipeline---Page-1--1.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2017/11/Face-Recognition-Pipeline---Page-1--1.png" alt="Demystifying Face Recognition I: Introduction"&gt;&lt;p&gt;In Web can be find many articles and research paper about Face Recognition (just look at &lt;a href="https://scirate.com/search?utf8=%E2%9C%93&amp;amp;q=Face+recognition"&gt;scirate&lt;/a&gt;) . Most of them introduce or explain some new technique and compare it to baseline, without going into details about choosing elements of pipeline. There also exist some Open-Source, ready to use library for Face-Recognition, where some of them achieve state-of-the-art results. Nice examples are &lt;a href="http://cmusatyalab.github.io/openface/"&gt;OpenFace&lt;/a&gt;, &lt;a href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html"&gt;DLib&lt;/a&gt; or &lt;a href="https://github.com/davidsandberg/facenet"&gt;FaceNet&lt;/a&gt;. But what if it turns out that the algorithm does not meet our expectations, what are the method to boost it, what helpful method exist? And how to properly investigate the algorithm to get the answer, is the algorithm is certainly better than the previously used? There is not much systematized information about it, just many papers, each with different pipeline.&lt;/p&gt;
&lt;p&gt;In this series of blog-post, we would like to change it by investigating state-of-art technique available until 2017. We will show new method which enable to boost the performance, verify the researcher's proposals under controlled test conditions and answer some questions that may not only bother me (hopefully, it will sth like &lt;code&gt;Myth Buster&lt;/code&gt; for Face Recognition). For last years there were many proposition targeted &lt;strong&gt;Face Recognition&lt;/strong&gt;, which we want to test, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TripletLoss&lt;/li&gt;
&lt;li&gt;CenterLoss&lt;/li&gt;
&lt;li&gt;LargeMarginSoftMax&lt;/li&gt;
&lt;li&gt;L2 normalization&lt;/li&gt;
&lt;li&gt;Cosine-Product vs Inner-Product&lt;/li&gt;
&lt;li&gt;Face Specific Augmentation&lt;/li&gt;
&lt;li&gt;Learning using 3D model&lt;/li&gt;
&lt;li&gt;Multi-Task Loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We hope that such evaluation would be even helpful for other tasks too, like Image Retrieval or One-Shot Classification as these topics are related to Face Recognition really closely. More about idea of this blog-series later, firstly let's look closer into benchmarks of Face Recognition. Thanks to them we can verify if new ideas can really boost the accuracy of overall system.&lt;/p&gt;
&lt;h2 id="leadingtechniquestotestthequalityoffacialrecognitionalgorithms"&gt;Leading techniques to test the quality of facial recognition algorithms&lt;/h2&gt;
&lt;p&gt;Facial recognition has been present in machine learning for a long time, but only since 2008 the progress in quality systems is rapidly increasing. The cause of technology development was primarily the publication of a benchmark named &lt;a href="http://"&gt;LFW&lt;/a&gt; (Labeled Faces in the Wild), which was distinguished primarily by sharing photos taken under uncontrolled conditions. The main test is based on Pair-Matching that is to compare the photos of two people and to judge whether it is the same or another person. Nowadays many methods achieve result close to perfection, ~99.5%. However even such results does not guarantee high performance in other, production condition. This is why the extension of LFW was proposed, named &lt;a href="http://"&gt;BLUFR&lt;/a&gt;. It contains two protocols: verification at fixed FAR (False-Acceptance-Rate) with 50 mln pairs and identification protocol (which is more production-realistic case).&lt;/p&gt;
&lt;p&gt;In 2015 another benchmark was proposes, exactly &lt;a href="http://"&gt;IARPA Janus Benchmark A&lt;/a&gt;. In terms on benchmark protocol, there are the same like in BLUFR, but there are based on &lt;code&gt;template&lt;/code&gt;. Each &lt;code&gt;template&lt;/code&gt; is created based on several The main difference is in image quality and difficulty. Also, in test images are frames extracted from video, which have much lower quality than images from camera. The authors also proposed different idea of testing by creating the &lt;code&gt;template&lt;/code&gt; for each person instead of testing similarity of each image of person independently. The creation of &lt;code&gt;template&lt;/code&gt; lies in the user's gesture, who can choose its own method for feature merging (like min, max or mean of feature).&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://localhost:2368/content/images/2017/11/janus_template.png"&gt;
&lt;/p&gt;
&lt;p&gt;Additonal, in 2017 the extension of JANUS-A was introduced, &lt;a href="http://"&gt;Janus Benchmark-B Face Dataset&lt;/a&gt;. Despite of increasing number of test images, the new protocols was introduced, which have more test scenario in comparing images and video-frames and also new face clustering protocol.&lt;/p&gt;
&lt;p&gt;The last face benchmark is &lt;a href="http://"&gt;MegaFace&lt;/a&gt;. As name suggest, this is large scale benchmark of Face Recognition (like ImageNet for Image Classification), containing over 1M images (much bigger than LFW and JANUS benchmark). The main idea is having 3 different dataset, distractors as main gallery dataset, &lt;code&gt;FaceScrub&lt;/code&gt; used for testing algorithm in normal condition and &lt;code&gt;FGNet&lt;/code&gt; used for testing algorithm in age-invariant settings. Like other knows benchmarks, it contain two protocols: verification (over 4 bilion pairs) and identification. In case of &lt;strong&gt;Challenge 1&lt;/strong&gt;, the researcher can choose from two variant based on data size set (Small, &amp;lt; 0.5M, Large &amp;gt; 0.5M of images). In &lt;strong&gt;Challenge 2&lt;/strong&gt;, which was introduced in 2016, each of the competitor have the same training database, containing ~5M images. The aim of that idea is testing the network architectures and algorithm, not the training database (like in case of &lt;strong&gt;Challenge 1&lt;/strong&gt; where everyone can use their own database).&lt;/p&gt;
&lt;p&gt;But how the results of benchmark compare to each other, does improvement in one test generalize to others? Very good compilation of results from many benchmark (not ony introduced above) is presented in paper &lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;. Authors include inter alia: &lt;a href="https://www.cs.tau.ac.il/~wolf/ytfaces/"&gt;YTF&lt;/a&gt;, &lt;a href="http://ieeexplore.ieee.org/document/4587572/"&gt;YTC&lt;/a&gt;, &lt;a href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html"&gt;Multi-PIE&lt;/a&gt;, &lt;a href="http://bcsiriuschen.github.io/CARC/"&gt;CACD-VS&lt;/a&gt;, &lt;a href="http://www.cbsr.ia.ac.cn/english/NIR-VIS-2.0-Database.html"&gt;CASIA 2.0 NIR-VIS&lt;/a&gt;, &lt;a href="http://ieeexplore.ieee.org/document/6805594/"&gt;Celebrity-1000 &lt;/a&gt;). Analyzing the results, look like the improvement generalize over most of the benchmark. But some of them better show even small improvement of algorithms than others. For example, having better model for about 0.5% in LFW can give boost of even 20% in BLUFR. If we want to see any, even a little improvement in our model, we should choose harder benchmark, even BLUFR.&lt;/p&gt;
&lt;h2 id="modernfacerecognitiontechnique"&gt;Modern Face Recognition Technique&lt;/h2&gt;
&lt;p&gt;The main method for Face Recognition are based on Deep Learning. The researchers are racing in ways to improve quality of system using bigger training sets, new architectures or changing a loss function. At present, the best face recognition technique is &lt;a href="http://vocord.com/"&gt;Vocoord&lt;/a&gt;, the winner of identification protocol in MegaFace and second best based on &lt;a href="https://www.nist.gov/sites/default/files/documents/2017/08/25/frvt_report_2017_08_25.pdf"&gt;NIST&lt;/a&gt;. Unfortunately we do not know any details about getting such high score.&lt;br&gt;
But there are many researcher that unveil details of their method, some of them even get &amp;gt; 99.5% on LFW. Some of them operate on database having ~ 2M images and multiple neural architectures. Others propose changing pipeline (ex. by data preparation) or adding new loss function (ex. CenterLoss). However, most of them show incremental increase of performance using their own pipeline, where each of them have different ways for ex. preprocessing. Such researches are hard to compare and does not bring us closer to achieving even better results in the future because we can not draw concrete conclusions about the learning process, for ex:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how face should be preprocessed?&lt;/li&gt;
&lt;li&gt;which data augmentation technique helps?&lt;/li&gt;
&lt;li&gt;which additional features in architectures helps?&lt;/li&gt;
&lt;li&gt;which loss function are best?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is because every scientist uses his or her concept of improving the model, which not always aim to achieve best possible results (as it involve interaction with many variables like database or architecture) but to show the rightness of the thesis. This is obviously an understandable approach, because it is a science. However, practitioners would like to know the limits of current technology of Face Recognition by merging multiple ideas from researchers. It is worth to verify certain theses under controlled conditions so that all test algorithms have equal chances. Many private company have such knowledge, but they does not reveal such secrets.&lt;/p&gt;
&lt;p&gt;In order to get acquainted with the current results on the LFW and YTF benchmarks, the table from &lt;a href="https://ydwen.github.io/papers/WenCVPR17.pdf"&gt;SphereFace&lt;/a&gt; is presented. It is interesting to note that the size of the database used for learning and the number of neural networks used are also given.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://localhost:2368/content/images/2017/11/sphereface.png"&gt;
&lt;/p&gt;
&lt;p&gt;These are not all available results, but they give us an overall view of the accuracy of the algorithms. Currently the best result on LFW is &lt;strong&gt;99.83%&lt;/strong&gt;, obtained by company named &lt;strong&gt;Glasix&lt;/strong&gt;. They provide following description of method:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We followed the Unrestricted, Labeled Outside Data protocol. The ratio of White to Black to Asian is 1:1:1 in the train set, which contains 100,000 individuals and 2 million face images. We spent a week training the networks which contain a improved resnet34 layer and a improved triplet loss layer on a Dual Maxwell-Gtx titan x machine with a four-stage training method.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://vis-www.cs.umass.edu/lfw/lfw_unrestricted_labeled_zm.png"&gt;
&lt;/p&gt;
&lt;p&gt;If you want to see more results from benchmark, look here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://vis-www.cs.umass.edu/lfw/results.html"&gt;LFW&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://megaface.cs.washington.edu/results/facescrub.html"&gt;MegaFace&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="aimofseries"&gt;Aim of series&lt;/h2&gt;
&lt;p&gt;The main aim of the series of post will be creating the full algorithm for Face Recognition, which will be having high results on public benchmarks (using Deep Learning). But the main target will be test on &lt;a href="http://megaface.cs.washington.edu/participate/challenge.html"&gt;MegaFace Challange 1 - Small&lt;/a&gt; and&lt;a href="http://megaface.cs.washington.edu/participate/challenge2.html"&gt; MegaFace Challange 2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To achieve very competitive results, here following ideas will be tested:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;preprocessing of data&lt;/li&gt;
&lt;li&gt;data augmentation technique&lt;/li&gt;
&lt;li&gt;loss functions&lt;/li&gt;
&lt;li&gt;optimization algorithm&lt;/li&gt;
&lt;li&gt;choosing the NN architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, at the end of the day, we will learn what pipeline to build to maximize model quality in face recognition tasks. We think that this series will be pretty long as every month new paper come out or new ideas are appearing. The beginning of series will be rather checking well known ideas but the farther we go the more research we will see there. We will be also testing some recently released ideas about Face-Recognition.&lt;br&gt;
In order to be able to draw conclusions from experiments, limitations and initial assumptions will be made to facilitate the analysis of the results.&lt;/p&gt;
&lt;p&gt;Limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;algorithms will be working at &lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt; (0.5M images, 10k individuals)&lt;/li&gt;
&lt;li&gt;90% of database it used for training, 10% for validation&lt;/li&gt;
&lt;li&gt;while testing, only single features will be extracted from sinlge image( so there will be nothing like &lt;a href="https://github.com/happynear/NormFace/blob/master/MirrorFace.md"&gt;mirror-trick&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;only one instance of model will be used (so there will be no feature merging from multiple model)&lt;/li&gt;
&lt;li&gt;start Learning Rate will be chosen from set: &lt;code&gt;0.1, 0.04, 0.01, 0.001&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;for reducing the LR, the detection of &lt;code&gt;Plateau&lt;/code&gt; will be used&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Initial assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;architectures will be using CASIA database align using &lt;a href="https://kpzhang93.github.io/papers/spl.pdf"&gt;MTCNN&lt;/a&gt; algorithm&lt;/li&gt;
&lt;li&gt;basic Data Augmentation technique will be mirror&lt;/li&gt;
&lt;/ul&gt;
 &lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://localhost:2368/content/images/2017/11/sprite_image_69_w_111_h_130.png"&gt;
&lt;/p&gt;
&lt;p&gt;The size of the database as well as the input images has been selected so as to enable high quality methods, while reducing the time it takes. The number of experiments needed to achieve the final result is enormous, and the computational power is limited.&lt;/p&gt;
&lt;p&gt;As a primary determinant of method quality, two results will be considered:&lt;br&gt;
LFW, LFW-BLUFR (both of them share features from same images). Additionally for best models the more complicated tests will be conducted: on IARPA Janus Benchmark-A and MegaFace. The &lt;code&gt;template&lt;/code&gt; at IJB-A will be created by taking the mean value.&lt;/p&gt;
&lt;p&gt;Each of the experiments will be compared to &lt;strong&gt;baseline&lt;/strong&gt;, the selected method (data-&amp;gt; architecture-&amp;gt; loss), which achieved its result using quite simple methods. This will allow us to evaluate whether the new proposed technology affects the quality of the algorithm positively. However, such an approach is not perfect and sometimes it may happen that the combination of several techniques only reflects the real impact of each. Unfortunately, such results can be missed. So when all the experiments will be done, the large-scale experiment will be conducted (DA or different loss with different scales) to get the best possible result. But earlier we want to sift through these techniques (ex. loss functions, which dozens have been proposed recently), which do not affect the final result to a large extent. In addition, we would like to test our own ideas and see if they make sense.&lt;/p&gt;
&lt;p&gt;The posts of each type of experiment will be in the form of a report that will reveal and analyze the results. The length of each post will depend on the subject and the number of experiments needed to confirm or refute the thesis. We expect that, for example, for the purpose of loss functions there will be about 4-5 posts.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/11/Face-Recognition-Pipeline---Page-1-.png" alt="Demystifying Face Recognition I: Introduction"&gt;&lt;br&gt;
We will start experiements from the begginig of all process, like in above picture. First post will be mainly about data and it preparation. Then we will take first try to get better accuracy by using more Data Augumentation technique. Next will be introducing some additional small changes in network architectures in idea of increasing capasity. After that we will examine many loss function, mainly based on single image (and maybe with pair and triple of image). After that we will be again ready for choosing the best architecture for our purpose, with all the knowledge gathered earlier. We hope to get better results than state-of-the-art.&lt;/p&gt;
&lt;p&gt;It's enough in the introduction, we hope everything is clear. In the next post we will look at the creation of &lt;strong&gt;baseline&lt;/strong&gt;, a reference to further experiments.&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://vis-www.cs.umass.edu/lfw/lfw_update.pdf"&gt;Labeled Faces in the Wild: Updates and New Reporting Procedures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cbsr.ia.ac.cn/users/scliao/papers/Liao-IJCB14-BLUFR.pdf"&gt;A Benchmark Study of Large-scale Unconstrained Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pdfs.semanticscholar.org/140c/95e53c619eac594d70f6369f518adfea12ef.pdf"&gt;Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA Janus Benchmark A&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://biometrics.cse.msu.edu/Publications/Face/Whitelametal_IARPAJanusBenchmark-BFaceDataset_CVPRW17.pdf"&gt;IARPA Janus Benchmark-B Face Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://megaface.cs.washington.edu/KemelmacherMegaFaceCVPR16.pdf"&gt;The MegaFace Benchmark: 1 Million Faces for Recognition at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Nech_Level_Playing_Field_2017_CVPR_supplemental.pdf"&gt;Level Playing Field for Million Scale Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1511.02683.pdf"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nist.gov/sites/default/files/documents/2017/08/25/frvt_report_2017_08_25.pdf"&gt;Ongoing Face Recognition Vendor Test (FRVT) Part 1: Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ydwen.github.io/papers/WenCVPR17.pdf"&gt;SphereFace: Deep Hypersphere Embedding for Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kpzhang93.github.io/papers/spl.pdf"&gt;Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item></channel></rss>