<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>BLCV - Bartosz Ludwiczuk Computer Vision</title><description>Computer Vision Blog and Consulting</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>BLCV - Bartosz Ludwiczuk Computer Vision</title><link>http://localhost:2368/</link></image><generator>Ghost 1.15</generator><lastBuildDate>Fri, 10 Nov 2017 12:36:22 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Demystifying Face Recognition III: Noise</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;In last years there were introduced many dataset for Face Recognition, named several:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt;: 0.5M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vintage.winklerbros.net/facescrub.html"&gt;FaceScrub&lt;/a&gt;: 0.1M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.msceleb.org/celeb1m/dataset"&gt;MsCeleb&lt;/a&gt;: 10M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.umdfaces.io"&gt;UMD&lt;/a&gt;: 0.38M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/"&gt;VGG&lt;/a&gt;: 2.6M of images&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main difference between them are the number of images and identities. But&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/2017/11/09/demystifying-face-recognition-iii-noise/</link><guid isPermaLink="false">5a047a1a1eb5111c6fa985dc</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Thu, 09 Nov 2017 16:15:35 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/logo-1.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2017/11/logo-1.png" alt="Demystifying Face Recognition III: Noise"&gt;&lt;p&gt;In last years there were introduced many dataset for Face Recognition, named several:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt;: 0.5M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vintage.winklerbros.net/facescrub.html"&gt;FaceScrub&lt;/a&gt;: 0.1M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.msceleb.org/celeb1m/dataset"&gt;MsCeleb&lt;/a&gt;: 10M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.umdfaces.io"&gt;UMD&lt;/a&gt;: 0.38M of images&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/"&gt;VGG&lt;/a&gt;: 2.6M of images&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main difference between them are the number of images and identities. But they're also different in sth like &lt;code&gt;signal-to-noise ratio&lt;/code&gt;, where we mean that ratio between correctly labeled images to all collected images. We do not know such value for any provided database, but some researcher provided the clean-list which provided images which was verified to be correct (by human or face-recognition algorithm). For example &lt;strong&gt;CASIA-WebFace&lt;/strong&gt; have 90% of correct images, in the other side &lt;strong&gt;VGG&lt;/strong&gt; have only 38% (rest of images have noisy labels). But what does it mean relative to accuracy of Face Recognition model? Does noise really hurt the performance in verification/identification protocol?&lt;/p&gt;
&lt;h2 id="noisydatabase"&gt;Noisy Database&lt;/h2&gt;
&lt;p&gt;First of all, let's make some literature review.&lt;br&gt;
One of the publication which we know about noise in data for Deep Learning is &lt;a href="https://arxiv.org/abs/1511.06789"&gt;The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition&lt;/a&gt;. The idea was simple: instead of marking pictures manually use information gathered in Web (metadata and search-engine). This enable to collect many more images than existed in current training dataset of several benchmarks. The results indicate that when training on original images and from web, the final accuracy is always higher (despite the fact, that the data in noisy).&lt;/p&gt;
&lt;p&gt;The first public face-recognition database with significant amount of noise was &lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"&gt;VGG&lt;/a&gt;. We will describe their method in more details. They collected 5M images from both Google and Bing Image Search. Then using Machine Learning algorithm with Fisher Vector Faces descriptor, half of images was removed. The last stage was manually filtering, which take 10 days (really hard work), ending with ~1M of images. Additional the images before manually filtering were provided, what end up with 2.6M. In experiments, Oxford team was using both dataset (named full and curated). And the bigger one leads to better results at all of their experiments, although it contain more noisy images than right one. Before jumping to any conclusion, let's take a closer look into manually filtering process by reading the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The aim of this final stage is to increase the purity (precision) of the data using human annotations. However, in order to make the annotation task less burdensome and hence avoid high annotation costs, annotators are aided by using automatic ranking once more. This time, however, a multi-way CNN is trained to discriminate between the 2,622 face identities using the AlexNet architecture; then the softmax scores are used to rank images within each identity set by decreasing likelihood of being an inlier. In order to accelerate the work of the annotators, the ranked images of each identity are displayed in blocks of 200 and annotators are asked to validate blocks as a whole. In particular, a block is declared good if approximate purity is greater than 95%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So filtering was done using block of images, which were accepted or refused by looking into purity of it. It means that if there is even some noise in block (more than 5%), then block is removed. To sum up, in 1.6M of noisy images there could be even ~1.4 of good labels. The similar situation is at other dataset like &lt;strong&gt;CASIA&lt;/strong&gt; or &lt;strong&gt;MsCeleb&lt;/strong&gt;: we are not sure that noise is really a noise, because most of time manually annotators does not investigate each of image independently. But from both of them researchers provided list of clean images by removing the noise manually (CASIA) or in automatic way (MsCeleb).&lt;/p&gt;
&lt;h2 id="experimentsoncasiawebface"&gt;Experiments on CASIA-WebFace&lt;/h2&gt;
&lt;p&gt;The first test will be based on &lt;strong&gt;UMD&lt;/strong&gt; database (as very clean database) and different version of &lt;strong&gt;CASIA-WebFace&lt;/strong&gt; database:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;full (0.5M of images)&lt;/li&gt;
&lt;li&gt;clean-v1 (0.47M, taken from &lt;a href="https://github.com/happynear/FaceVerification"&gt;Happy-Near page&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;clean-v2 (0.445M, taken from project &lt;a href="http://zhengyingbin.cc/ActiveAnnotationLearning"&gt;Face Recognition via Active Annotation and Learning&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idea behind using this database is testing algorithm on very clean database with smaller size (UMD) versus bigger and noisier database with some efforts of cleaning it.&lt;/p&gt;
&lt;p&gt;For training the network, we will be using the same setting like for baseline. Here are results.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="620" height="300" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/108.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;There is no clean conclusion from this experiments. First of all, we can see that delivered clean-list for CASIA-WebFace really have lower noise, because network trained using them have lower loss and higher validation accuracy. But this is the main difference, all network achieve similar score at LFW and BLUFR benchmarks. The highest score achieve the cleanest one, &lt;strong&gt;clean-v2&lt;/strong&gt;, but the difference is only ~1%. Maybe if dataset would contain more images, the bigger change would be noticed. &lt;strong&gt;UMD&lt;/strong&gt; database despite of being very clean, achieve the lowest score.&lt;/p&gt;
&lt;h2 id="experimentsonvggdatabase"&gt;Experiments on VGG-Database&lt;/h2&gt;
&lt;p&gt;As the first experiments does not make any clean conclusion, let's use &lt;strong&gt;VGG&lt;/strong&gt; database. As the level of noisy labels in VGG can be much bigger, here will be conducted  experiments using different ratio of noise (exactly images labeled as noise to all images in training set). We hope that it will show how different amount of noise impact the final accuracy (remembering that the noise in case of VGG maybe not a real noise). For coherence of results, in each experiment same validation dataset is used.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="750" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/112.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;Unfortunately, the results are again not conclusive. In contrast to earlier results, here more noise make result better. We think that it is because of noisy labels, which have a lot of true labels.&lt;/p&gt;
&lt;h2 id="experimentsusingrealnoise"&gt;Experiments using Real Noise&lt;/h2&gt;
&lt;p&gt;In such situation, we decided to carry out last experiment. Using the cleanest version of CASIA-WebFace we will be injecting real noise to data. The noise will be formed from images taken from VGG database with random labels. As we would like to have a small amount of noise in base training data, we will use &lt;strong&gt;clean-v2&lt;/strong&gt; list.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="750" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/114.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;Finally, this experiments provide us clear conclusion. The real noise, even in small amout, hurt the final accuracy. Although the model with 10% of noise achieve better score at LFW, the BLUFR protocol show the real difference. So we can conclude that is nice to have less than 10% of noise. Also strange situation happened with noise ratio of 50%: the network diverge, so we should lower the learning-rate.&lt;/p&gt;
&lt;p&gt;In the other hard, it is really intriguing that having the 80% of noisy labels still network can learn anything useful. The similar result was achieved by researcher in paper &lt;a href="https://arxiv.org/abs/1705.10694"&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;, even that the experiments have 99% of noise. The researcher have very novel conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep neural networks are able to learn from data that has been diluted by an arbitrary amount of noise.&lt;/li&gt;
&lt;li&gt;A sufficiently large training set is more important than a lower level of noise.&lt;/li&gt;
&lt;li&gt;Choosing good hyperparameters can allow conventional neural networks to operate in the regime of very high label noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For sure Deep Learning is amazing in handling the noisy data, but we cannot agree with second conclusion, already owning ~0.5M of data example. Having 0.4M of images with clean labels is better than any database with larger amount of images but with noise. Other pros of having clean and small database is faster convergence (as there is less training examples). In the other hand, when we are collecting our own dataset it is inevitable to have noisy labels. Our experiments show that data should not have more than 10% of real noise.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Look like noisy labels impact the final accuracy of model, so it is not a good idea to artificially make dataset bigger by injecting of noise (like a data augmentation method). Also, the cleaner dataset with enough training examples the better. It is contradict to conclusion from paper &lt;a href="https://arxiv.org/abs/1511.06789"&gt;The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition&lt;/a&gt;. It is because of different initial assumptions as their dataset have sizes: 6k, 10k, 20k, 50k. So each of them is much smaller that we used. In other hand, if our noisy dataset have sth like &amp;gt;90% of good labels, it is recommended to use it at training as well.&lt;br&gt;
In such situation we need to agree with conclusion from &lt;a href="https://arxiv.org/abs/1705.10694"&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;, with slight modification:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A sufficiently large training set is more important than a lower level of noise only if your original dataset have not enough data (which number depend on task, ex. Face-Recognition is ~0.4M)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Final conclusion:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cleaning the dataset can lead to better accuracy, but gain will be minimal if there were less than 10% of noise.&lt;/li&gt;
&lt;li&gt;If we have database which have only 5%-10% of noisy labels, it is advised to use it for training.&lt;/li&gt;
&lt;li&gt;Adding images with real noise is not beneficial.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ok, so test another aspect of preparing the dataset for Face-Recognition. The next question which will we ask about pipeline is:&lt;br&gt;
&lt;strong&gt;How the input image should be aligned before feeding it to network?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Learning Face Representation from Scratch”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vintage.winklerbros.net/Publications/icip2014a.pdf"&gt;A data-driven approach to cleaning large face datasets.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1607.08221"&gt;MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.01484v2"&gt;UMDFaces: An Annotated Face Dataset for Training Deep Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"&gt;VGG-Deep Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1511.06789"&gt;The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1705.10694"&gt;Deep Learning is Robust to Massive Label Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zhengyingbin.cc/ActiveAnnotationLearning"&gt;Face Recognition via Active Annotation and Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/happynear/FaceVerification"&gt;Happy-Near page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition II: Baseline</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="testofdifferentnetworkarchitectures"&gt;Test of different network architectures&lt;/h2&gt;
&lt;p&gt;According to assumptions, the database is chosen (CASIA-WebFace), input image is preprocessed (112x96, MTCNN), only mirror used as Data Augmentation and for learning the CrossEntropy loss will be used. The last lacking element in the pipeline is network architecture. Last years was really abounded in&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/2017/11/08/demystifying-face-recognition-ii-baseline/</link><guid isPermaLink="false">5a0357fdd370791075dc45cc</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Wed, 08 Nov 2017 19:16:59 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/faceresnet.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="testofdifferentnetworkarchitectures"&gt;Test of different network architectures&lt;/h2&gt;
&lt;img src="http://localhost:2368/content/images/2017/11/faceresnet.png" alt="Demystifying Face Recognition II: Baseline"&gt;&lt;p&gt;According to assumptions, the database is chosen (CASIA-WebFace), input image is preprocessed (112x96, MTCNN), only mirror used as Data Augmentation and for learning the CrossEntropy loss will be used. The last lacking element in the pipeline is network architecture. Last years was really abounded in many diverse ideas about creating the architectures like &lt;strong&gt;ResNet&lt;/strong&gt;, &lt;strong&gt;Inception&lt;/strong&gt; or &lt;strong&gt;DenseNet&lt;/strong&gt;. Additional, the community of Face Recognition was also introducing their own architectures like &lt;strong&gt;FaceResNet&lt;/strong&gt;, &lt;strong&gt;SphereNet&lt;/strong&gt;, &lt;strong&gt;LightCNN&lt;/strong&gt; or &lt;strong&gt;FudanNet&lt;/strong&gt;. Currently we will look closer to the later one as we know their performance and low computation requirements.&lt;br&gt;
We will also include some older architectures to see if it is really true then the new ones works much better than architectures from 2014 or earlier, we choose &lt;strong&gt;LeNet&lt;/strong&gt;, &lt;strong&gt;DeepID&lt;/strong&gt;,** DeepID2+** and &lt;strong&gt;CASIA&lt;/strong&gt;.&lt;br&gt;
This is not the final choice of the architecture, we just want to get a reasonable baseline, which will accompany us with all others test. There are so many test, because we want to make sure, that my current pipeline works well and if my implementation match results from papers.&lt;/p&gt;
&lt;h2 id="descriptionofarchitectures"&gt;Description of Architectures&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;LeNet&lt;/a&gt; - the most popular convolutional architecture. Input image: 28x24.&lt;br&gt;
​&lt;br&gt;
&lt;a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf"&gt;DeepID&lt;/a&gt; - One of the first specialized networks used for Face Recognition. Comparing to LeNet, it have more filters and final feature comes from merging data from two layers. Input image: 42x36.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1412.1265"&gt;DeepID2+&lt;/a&gt; - Extension of &lt;strong&gt;DeepID&lt;/strong&gt;, have much more number of filters and features size is now 512. Input image:56x48.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Casia-Net&lt;/a&gt; - Architecture proposed after success of &lt;strong&gt;VGG&lt;/strong&gt; and &lt;strong&gt;GoogLeNet&lt;/strong&gt;. It use concept of kernel 3x3 and Average Pooling.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1511.02683"&gt;Light-CNN&lt;/a&gt;- The author propose to use &lt;code&gt;MFM&lt;/code&gt;as a activation function, which is extension of &lt;code&gt;MaxOut&lt;/code&gt;. In his experiments it is better than &lt;code&gt;ReLU&lt;/code&gt;, &lt;code&gt;ELU&lt;/code&gt; or even &lt;code&gt;PReLU&lt;/code&gt;.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1611.08976"&gt;FaceResNet&lt;/a&gt; - Architecture proposed by author of CenterLoss and RangeLoss, which use residual connection, like in ResNet. But it does not use BatchNorm and replaces the &lt;code&gt;Relu&lt;/code&gt; activation functions with the &lt;code&gt;PRELu&lt;/code&gt; functions.&lt;br&gt;
​&lt;br&gt;
&lt;a href="https://arxiv.org/abs/1704.08063"&gt;SphereFace&lt;/a&gt; - New version of &lt;strong&gt;FaceResNet&lt;/strong&gt; which mainly replace each &lt;code&gt;MaxPool&lt;/code&gt; by &lt;code&gt;Convolution&lt;/code&gt; with stride equal to 2.&lt;br&gt;
​&lt;br&gt;
&lt;a href="http://www.yugangjiang.info/publication/icmr17-face.pdf"&gt;Fudan-Arch&lt;/a&gt; -Idea of &lt;strong&gt;FaceResNet&lt;/strong&gt; but with &lt;code&gt;Batchnorm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Most of the above architecture have also &lt;a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"&gt;DropOut&lt;/a&gt; inside, other have own regularization method. If we just want to replicate the results as stated at paper, we would still not be able to compare such results to each other because of different settings. This is why we will completely ignore any special regularization method (like &lt;a href="https://ydwen.github.io/papers/WenECCV16.pdf"&gt;CenterLoss&lt;/a&gt;) and here will be two experiments for each architecture: with and without DropOut. This would also help to validate the current implementation with the results from papers.&lt;br&gt;
To evaluate each network we will use its qualitative results (accuracy and loss value) and time-to-score. The model of each architect was chosen based on the model with the lowest validation loss, ie the result of the LFW did not affect the choice of the model, even though the models achieved better results in another epoch.&lt;/p&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p align="center"&gt;&lt;b&gt;CASIA Training and LFW&lt;/b&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="840" height="1020" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/92.embed"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;iframe width="800" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/98.embed"&gt;&lt;/iframe&gt;
&lt;iframe width="800" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/97.embed"&gt;&lt;/iframe&gt;
&lt;p&gt;First of all, let's look closer to architectures with DropOut. There is not clean winner here, &lt;strong&gt;Fudan-Full&lt;/strong&gt;, &lt;strong&gt;SphereFace64&lt;/strong&gt; and &lt;strong&gt;Light-CNN29&lt;/strong&gt; are overall comparable, but each of them dominate at one of the given benchmark (validation loss, LFW, BLUFR). Very close to them in &lt;strong&gt;FaceResNet&lt;/strong&gt;, which was training much faster. It is very interesting that many network achieve &amp;gt; 97% at LFW, however BLUFR protocols show us the real difference in quality. For example, difference in 0.7% at LFW between &lt;strong&gt;CASIA&lt;/strong&gt; and &lt;strong&gt;SphereFace64&lt;/strong&gt; translate to 16% in BLUFR-FAR 1%.&lt;br&gt;
What about architectures without any regularization? Here the clear winner is &lt;strong&gt;Fudan-Full&lt;/strong&gt;, followed by &lt;strong&gt;SphereFace64&lt;/strong&gt; and &lt;strong&gt;FaceResNet&lt;/strong&gt;. From the intuition, it look like that &lt;code&gt;BatchNorm&lt;/code&gt; at &lt;strong&gt;Fudan-Ful&lt;/strong&gt;l helped at lot as it behaves like a regularizator. From such comparison we can also deduce which architecture is good for testing any new Data Augmentation technique or new loss, because it would show us even small gain. In our case it is &lt;strong&gt;Light-CNN29&lt;/strong&gt; which overfit a lot.&lt;/p&gt;
&lt;p&gt;For detailed analyse we choose best models: &lt;strong&gt;FaceResNet&lt;/strong&gt;, &lt;strong&gt;Light-CNN29&lt;/strong&gt;, &lt;strong&gt;SphereFace64&lt;/strong&gt; and &lt;strong&gt;FudanFull&lt;/strong&gt;.&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;IJB-A&lt;/b&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="800" height="260" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/106.embed"&gt;&lt;/iframe&gt;
 &lt;/p&gt;
&lt;p&gt;Overall here &lt;strong&gt;Light-CNN29&lt;/strong&gt; is the winner, which lose at only Rank-1 benchmark. But &lt;strong&gt;SphereFace64&lt;/strong&gt; is breathing down its neck by being just slightly worse. The results from &lt;strong&gt;FundanFull&lt;/strong&gt; are really bad, not sure what is the reason for that.&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;Mega Face&lt;/b&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;iframe width="500" height="200" frameborder="0" scrolling="no" src="https://plot.ly/~melgor89/104.embed" align="center"&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;In MageFace, identification protocol is winner by &lt;strong&gt;FudanFull&lt;/strong&gt; while verification protocol is taken by &lt;strong&gt;Light-CNN29&lt;/strong&gt; (where &lt;strong&gt;FudanFull&lt;/strong&gt; is again the weakest)&lt;/p&gt;
&lt;h2 id="baselinemodel"&gt;Baseline Model&lt;/h2&gt;
&lt;p&gt;Summarizing, if we want to choose best architecture among tested, the &lt;strong&gt;Light-CNN29&lt;/strong&gt; would be the best with &lt;strong&gt;Sphere64&lt;/strong&gt; just right behind. &lt;strong&gt;FudanFull&lt;/strong&gt; works nice, but in some scenario its accuracy is too low. This is our podium. Looking closer to the this architectures, the common thing is using residual connection. But they vary at activation function, using &lt;code&gt;Pool&lt;/code&gt; vs &lt;code&gt;Convolution&lt;/code&gt; with stride equal 2 and using &lt;code&gt;BatchNorm&lt;/code&gt;. So maybe they are not best possible architectures? We will leave this question for future tests.&lt;/p&gt;
&lt;p&gt;When we compare the results from current implementation with the results from paper, most of them matched target accuracy. The only exception is &lt;strong&gt;SphereFace&lt;/strong&gt;, which without DropOut overfit, although the original version does not have it.&lt;/p&gt;
&lt;p&gt;When we compare the time needed for getting best results, this is definitely the best place for FaceResNet, which is only slightly weaker than the best model, but it learned almost 3x shorter. This is why &lt;strong&gt;FaceResNet&lt;/strong&gt; is chosen as baseline architecture. He will accompany us throughout the series named &lt;strong&gt;Face Recognition&lt;/strong&gt;. Specifically, both &lt;strong&gt;FaceResNet&lt;/strong&gt; will be used, depending of scenario: when we will be reducing overfiting by new technique, we will use raw architecture, in other case DropOut will be used.&lt;/p&gt;
&lt;h2 id="whatnext"&gt;What next?&lt;/h2&gt;
&lt;p&gt;Looking into the results it look like that getting &lt;strong&gt;~98&lt;/strong&gt;% on LFW using only basic technique for learning is easy. This results would be among the best 3 years ago, but currently it is ~1.5% behind state-of-the-art. In MegaFace in even worse, because our results is &lt;strong&gt;20%&lt;/strong&gt; lower using same dataset.&lt;br&gt;
How we can boost accuracy of our model? A lot of researcher propose their own technique, but will they work in our case? What boost can we gain? We will learn this in the next post, and in the near future we will look at the aspect of noise in the learning data.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;Gradient-Based Learning Applied to Document Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf"&gt;Deep Learning Face Representation from Predicting 10,000 Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1412.1265"&gt;Deeply learned face representations are sparse, selective, and robust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1411.7923"&gt;Learning Face Representation from Scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ydwen.github.io/papers/WenECCV16.pdf"&gt;A Discriminative Feature Learning Approach for Deep Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.08976"&gt;Range Loss for Deep Face Recognition with Long-tail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1704.08063"&gt;SphereFace: Deep Hypersphere Embedding for Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.yugangjiang.info/publication/icmr17-face.pdf"&gt;Multi-task Deep Neural Network for Joint Face Recognition and Facial Aribute Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item><item><title>Demystifying Face Recognition I: Introduction</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;In Web can be find many articles and research paper about Face Recognition (just look at &lt;a href="https://scirate.com/search?utf8=%E2%9C%93&amp;amp;q=Face+recognition"&gt;scirate&lt;/a&gt;) . Most of them introduce or explain some new technique and compare it to baseline, without going into details about choosing elements of pipeline. There also exist some Open-Source, ready to use library for&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/2017/11/07/demystifying-face-recognition-i-introduction/</link><guid isPermaLink="false">5a02154c9bbba6631c618c5f</guid><category>face-recogition</category><dc:creator>Bartosz Ludwiczuk</dc:creator><pubDate>Tue, 07 Nov 2017 20:20:59 GMT</pubDate><media:content url="http://localhost:2368/content/images/2017/11/Face-Recognition-Pipeline---Page-1--1.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2017/11/Face-Recognition-Pipeline---Page-1--1.png" alt="Demystifying Face Recognition I: Introduction"&gt;&lt;p&gt;In Web can be find many articles and research paper about Face Recognition (just look at &lt;a href="https://scirate.com/search?utf8=%E2%9C%93&amp;amp;q=Face+recognition"&gt;scirate&lt;/a&gt;) . Most of them introduce or explain some new technique and compare it to baseline, without going into details about choosing elements of pipeline. There also exist some Open-Source, ready to use library for Face-Recognition, where some of them achieve state-of-the-art results. Nice examples are &lt;a href="http://cmusatyalab.github.io/openface/"&gt;OpenFace&lt;/a&gt;, &lt;a href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html"&gt;DLib&lt;/a&gt; or &lt;a href="https://github.com/davidsandberg/facenet"&gt;FaceNet&lt;/a&gt;. But what if it turns out that the algorithm does not meet our expectations, what are the method to boost it, what helpful method exist? And how to properly investigate the algorithm to get the answer, is the algorithm is certainly better than the previously used? There is not much systematized information about it, just many papers, each with different pipeline.&lt;/p&gt;
&lt;p&gt;In this series of blog-post, we would like to change it by investigating state-of-art technique available until 2017. We will show new method which enable to boost the performance, verify the researcher's proposals under controlled test conditions and answer some questions that may not only bother me (hopefully, it will sth like &lt;code&gt;Myth Buster&lt;/code&gt; for Face Recognition). For last years there were many proposition targeted &lt;strong&gt;Face Recognition&lt;/strong&gt;, which we want to test, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TripletLoss&lt;/li&gt;
&lt;li&gt;CenterLoss&lt;/li&gt;
&lt;li&gt;LargeMarginSoftMax&lt;/li&gt;
&lt;li&gt;L2 normalization&lt;/li&gt;
&lt;li&gt;Cosine-Product vs Inner-Product&lt;/li&gt;
&lt;li&gt;Face Specific Augmentation&lt;/li&gt;
&lt;li&gt;Learning using 3D model&lt;/li&gt;
&lt;li&gt;Multi-Task Loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We hope that such evaluation would be even helpful for other tasks too, like Image Retrieval or One-Shot Classification as these topics are related to Face Recognition really closely. More about idea of this blog-series later, firstly let's look closer into benchmarks of Face Recognition. Thanks to them we can verify if new ideas can really boost the accuracy of overall system.&lt;/p&gt;
&lt;h2 id="leadingtechniquestotestthequalityoffacialrecognitionalgorithms"&gt;Leading techniques to test the quality of facial recognition algorithms&lt;/h2&gt;
&lt;p&gt;Facial recognition has been present in machine learning for a long time, but only since 2008 the progress in quality systems is rapidly increasing. The cause of technology development was primarily the publication of a benchmark named &lt;a href="http://"&gt;LFW&lt;/a&gt; (Labeled Faces in the Wild), which was distinguished primarily by sharing photos taken under uncontrolled conditions. The main test is based on Pair-Matching that is to compare the photos of two people and to judge whether it is the same or another person. Nowadays many methods achieve result close to perfection, ~99.5%. However even such results does not guarantee high performance in other, production condition. This is why the extension of LFW was proposed, named &lt;a href="http://"&gt;BLUFR&lt;/a&gt;. It contains two protocols: verification at fixed FAR (False-Acceptance-Rate) with 50 mln pairs and identification protocol (which is more production-realistic case).&lt;/p&gt;
&lt;p&gt;In 2015 another benchmark was proposes, exactly &lt;a href="http://"&gt;IARPA Janus Benchmark A&lt;/a&gt;. In terms on benchmark protocol, there are the same like in BLUFR, but there are based on &lt;code&gt;template&lt;/code&gt;. Each &lt;code&gt;template&lt;/code&gt; is created based on several The main difference is in image quality and difficulty. Also, in test images are frames extracted from video, which have much lower quality than images from camera. The authors also proposed different idea of testing by creating the &lt;code&gt;template&lt;/code&gt; for each person instead of testing similarity of each image of person independently. The creation of &lt;code&gt;template&lt;/code&gt; lies in the user's gesture, who can choose its own method for feature merging (like min, max or mean of feature).&lt;/p&gt;
&lt;p align="Template comparison at JANUS Benchmark"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://localhost:2368/content/images/2017/11/janus_template.png"&gt;
&lt;/p&gt;
&lt;p&gt;Additonal, in 2017 the extension of JANUS-A was introduced, &lt;a href="http://"&gt;Janus Benchmark-B Face Dataset&lt;/a&gt;. Despite of increasing number of test images, the new protocols was introduced, which have more test scenario in comparing images and video-frames and also new face clustering protocol.&lt;/p&gt;
&lt;p&gt;The last face benchmark is &lt;a href="http://"&gt;MegaFace&lt;/a&gt;. As name suggest, this is large scale benchmark of Face Recognition (like ImageNet for Image Classification), containing over 1M images (much bigger than LFW and JANUS benchmark). The main idea is having 3 different dataset, distractors as main gallery dataset, &lt;code&gt;FaceScrub&lt;/code&gt; used for testing algorithm in normal condition and &lt;code&gt;FGNet&lt;/code&gt; used for testing algorithm in age-invariant settings. Like other knows benchmarks, it contain two protocols: verification (over 4 bilion pairs) and identification. In case of &lt;strong&gt;Challenge 1&lt;/strong&gt;, the researcher can choose from two variant based on data size set (Small, &amp;lt; 0.5M, Large &amp;gt; 0.5M of images). In &lt;strong&gt;Challenge 2&lt;/strong&gt;, which was introduced in 2016, each of the competitor have the same training database, containing ~5M images. The aim of that idea is testing the network architectures and algorithm, not the training database (like in case of &lt;strong&gt;Challenge 1&lt;/strong&gt; where everyone can use their own database).&lt;/p&gt;
&lt;p&gt;But how the results of benchmark compare to each other, does improvement in one test generalize to others? Very good compilation of results from many benchmark (not ony introduced above) is presented in paper &lt;a href="https://arxiv.org/abs/1511.02683"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;. Authors include inter alia: &lt;a href="https://www.cs.tau.ac.il/~wolf/ytfaces/"&gt;YTF&lt;/a&gt;, &lt;a href="http://ieeexplore.ieee.org/document/4587572/"&gt;YTC&lt;/a&gt;, &lt;a href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html"&gt;Multi-PIE&lt;/a&gt;, &lt;a href="http://bcsiriuschen.github.io/CARC/"&gt;CACD-VS&lt;/a&gt;, &lt;a href="http://www.cbsr.ia.ac.cn/english/NIR-VIS-2.0-Database.html"&gt;CASIA 2.0 NIR-VIS&lt;/a&gt;, &lt;a href="http://ieeexplore.ieee.org/document/6805594/"&gt;Celebrity-1000 &lt;/a&gt;). Analyzing the results, look like the improvement generalize over most of the benchmark. But some of them better show even small improvement of algorithms than others. For example, having better model for about 0.5% in LFW can give boost of even 20% in BLUFR. If we want to see any, even a little improvement in our model, we should choose harder benchmark, even BLUFR.&lt;/p&gt;
&lt;h2 id="modernfacerecognitiontechnique"&gt;Modern Face Recognition Technique&lt;/h2&gt;
&lt;p&gt;The main method for Face Recognition are based on Deep Learning. The researchers are racing in ways to improve quality of system using bigger training sets, new architectures or changing a loss function. At present, the best face recognition technique is &lt;a href="http://vocord.com/"&gt;Vocoord&lt;/a&gt;, the winner of identification protocol in MegaFace and second best based on &lt;a href="https://www.nist.gov/sites/default/files/documents/2017/08/25/frvt_report_2017_08_25.pdf"&gt;NIST&lt;/a&gt;. Unfortunately we do not know any details about getting such high score.&lt;br&gt;
But there are many researcher that unveil details of their method, some of them even get &amp;gt; 99.5% on LFW. Some of them operate on database having ~ 2M images and multiple neural architectures. Others propose changing pipeline (ex. by data preparation) or adding new loss function (ex. CenterLoss). However, most of them show incremental increase of performance using their own pipeline, where each of them have different ways for ex. preprocessing. Such researches are hard to compare and does not bring us closer to achieving even better results in the future because we can not draw concrete conclusions about the learning process, for ex:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how face should be preprocessed?&lt;/li&gt;
&lt;li&gt;which data augmentation technique helps?&lt;/li&gt;
&lt;li&gt;which additional features in architectures helps?&lt;/li&gt;
&lt;li&gt;which loss function are best?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is because every scientist uses his or her concept of improving the model, which not always aim to achieve best possible results (as it involve interaction with many variables like database or architecture) but to show the rightness of the thesis. This is obviously an understandable approach, because it is a science. However, practitioners would like to know the limits of current technology of Face Recognition by merging multiple ideas from researchers. It is worth to verify certain theses under controlled conditions so that all test algorithms have equal chances. Many private company have such knowledge, but they does not reveal such secrets.&lt;/p&gt;
&lt;p&gt;In order to get acquainted with the current results on the LFW and YTF benchmarks, the table from &lt;a href="https://ydwen.github.io/papers/WenCVPR17.pdf"&gt;SphereFace&lt;/a&gt; is presented. It is interesting to note that the size of the database used for learning and the number of neural networks used are also given.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://localhost:2368/content/images/2017/11/sphereface.png"&gt;
&lt;/p&gt;
&lt;p&gt;These are not all available results, but they give us an overall view of the accuracy of the algorithms. Currently the best result on LFW is &lt;strong&gt;99.83%&lt;/strong&gt;, obtained by company named &lt;strong&gt;Glasix&lt;/strong&gt;. They provide following description of method:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We followed the Unrestricted, Labeled Outside Data protocol. The ratio of White to Black to Asian is 1:1:1 in the train set, which contains 100,000 individuals and 2 million face images. We spent a week training the networks which contain a improved resnet34 layer and a improved triplet loss layer on a Dual Maxwell-Gtx titan x machine with a four-stage training method.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://vis-www.cs.umass.edu/lfw/lfw_unrestricted_labeled_zm.png"&gt;
&lt;/p&gt;
&lt;p&gt;If you want to see more results from benchmark, look here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://vis-www.cs.umass.edu/lfw/results.html"&gt;LFW&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://megaface.cs.washington.edu/results/facescrub.html"&gt;MegaFace&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="aimofseries"&gt;Aim of series&lt;/h2&gt;
&lt;p&gt;The main aim of the series of post will be creating the full algorithm for Face Recognition, which will be having high results on public benchmarks (using Deep Learning). But the main target will be test on &lt;a href="http://megaface.cs.washington.edu/participate/challenge.html"&gt;MegaFace Challange 1 - Small&lt;/a&gt; and&lt;a href="http://megaface.cs.washington.edu/participate/challenge2.html"&gt; MegaFace Challange 2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To achieve very competitive results, here following ideas will be tested:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;preprocessing of data&lt;/li&gt;
&lt;li&gt;data augmentation technique&lt;/li&gt;
&lt;li&gt;loss functions&lt;/li&gt;
&lt;li&gt;optimization algorithm&lt;/li&gt;
&lt;li&gt;choosing the NN architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, at the end of the day, we will learn what pipeline to build to maximize model quality in face recognition tasks. We think that this series will be pretty long as every month new paper come out or new ideas are appearing. The beginning of series will be rather checking well known ideas but the farther we go the more research we will see there. We will be also testing some recently released ideas about Face-Recognition.&lt;br&gt;
In order to be able to draw conclusions from experiments, limitations and initial assumptions will be made to facilitate the analysis of the results.&lt;/p&gt;
&lt;p&gt;Limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;algorithms will be working at &lt;a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html"&gt;CASIA-WebFace&lt;/a&gt; (0.5M images, 10k individuals)&lt;/li&gt;
&lt;li&gt;90% of database it used for training, 10% for validation&lt;/li&gt;
&lt;li&gt;while testing, only single features will be extracted from sinlge image( so there will be nothing like &lt;a href="https://github.com/happynear/NormFace/blob/master/MirrorFace.md"&gt;mirror-trick&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;only one instance of model will be used (so there will be no feature merging from multiple model)&lt;/li&gt;
&lt;li&gt;start Learning Rate will be chosen from set: &lt;code&gt;0.1, 0.04, 0.01, 0.001&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;for reducing the LR, the detection of &lt;code&gt;Plateau&lt;/code&gt; will be used&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Initial assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;architectures will be using CASIA database align using &lt;a href="https://kpzhang93.github.io/papers/spl.pdf"&gt;MTCNN&lt;/a&gt; algorithm&lt;/li&gt;
&lt;li&gt;basic Data Augmentation technique will be mirror&lt;/li&gt;
&lt;/ul&gt;
 &lt;p align="center"&gt;
&lt;img alt="Demystifying Face Recognition I: Introduction" src="http://localhost:2368/content/images/2017/11/sprite_image_69_w_111_h_130.png"&gt;
&lt;/p&gt;
&lt;p&gt;The size of the database as well as the input images has been selected so as to enable high quality methods, while reducing the time it takes. The number of experiments needed to achieve the final result is enormous, and the computational power is limited.&lt;/p&gt;
&lt;p&gt;As a primary determinant of method quality, two results will be considered:&lt;br&gt;
LFW, LFW-BLUFR (both of them share features from same images). Additionally for best models the more complicated tests will be conducted: on IARPA Janus Benchmark-A and MegaFace. The &lt;code&gt;template&lt;/code&gt; at IJB-A will be created by taking the mean value.&lt;/p&gt;
&lt;p&gt;Each of the experiments will be compared to &lt;strong&gt;baseline&lt;/strong&gt;, the selected method (data-&amp;gt; architecture-&amp;gt; loss), which achieved its result using quite simple methods. This will allow us to evaluate whether the new proposed technology affects the quality of the algorithm positively. However, such an approach is not perfect and sometimes it may happen that the combination of several techniques only reflects the real impact of each. Unfortunately, such results can be missed. So when all the experiments will be done, the large-scale experiment will be conducted (DA or different loss with different scales) to get the best possible result. But earlier we want to sift through these techniques (ex. loss functions, which dozens have been proposed recently), which do not affect the final result to a large extent. In addition, we would like to test our own ideas and see if they make sense.&lt;/p&gt;
&lt;p&gt;The posts of each type of experiment will be in the form of a report that will reveal and analyze the results. The length of each post will depend on the subject and the number of experiments needed to confirm or refute the thesis. We expect that, for example, for the purpose of loss functions there will be about 4-5 posts.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2017/11/Face-Recognition-Pipeline---Page-1-.png" alt="Demystifying Face Recognition I: Introduction"&gt;&lt;br&gt;
We will start experiements from the begginig of all process, like in above picture. First post will be mainly about data and it preparation. Then we will take first try to get better accuracy by using more Data Augumentation technique. Next will be introducing some additional small changes in network architectures in idea of increasing capasity. After that we will examine many loss function, mainly based on single image (and maybe with pair and triple of image). After that we will be again ready for choosing the best architecture for our purpose, with all the knowledge gathered earlier. We hope to get better results than state-of-the-art.&lt;/p&gt;
&lt;p&gt;It's enough in the introduction, we hope everything is clear. In the next post we will look at the creation of &lt;strong&gt;baseline&lt;/strong&gt;, a reference to further experiments.&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://vis-www.cs.umass.edu/lfw/lfw_update.pdf"&gt;Labeled Faces in the Wild: Updates and New Reporting Procedures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cbsr.ia.ac.cn/users/scliao/papers/Liao-IJCB14-BLUFR.pdf"&gt;A Benchmark Study of Large-scale Unconstrained Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pdfs.semanticscholar.org/140c/95e53c619eac594d70f6369f518adfea12ef.pdf"&gt;Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA Janus Benchmark A&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://biometrics.cse.msu.edu/Publications/Face/Whitelametal_IARPAJanusBenchmark-BFaceDataset_CVPRW17.pdf"&gt;IARPA Janus Benchmark-B Face Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://megaface.cs.washington.edu/KemelmacherMegaFaceCVPR16.pdf"&gt;The MegaFace Benchmark: 1 Million Faces for Recognition at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Nech_Level_Playing_Field_2017_CVPR_supplemental.pdf"&gt;Level Playing Field for Million Scale Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1511.02683.pdf"&gt;A Light CNN for Deep Face Representation with Noisy Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nist.gov/sites/default/files/documents/2017/08/25/frvt_report_2017_08_25.pdf"&gt;Ongoing Face Recognition Vendor Test (FRVT) Part 1: Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ydwen.github.io/papers/WenCVPR17.pdf"&gt;SphereFace: Deep Hypersphere Embedding for Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kpzhang93.github.io/papers/spl.pdf"&gt;Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item></channel></rss>