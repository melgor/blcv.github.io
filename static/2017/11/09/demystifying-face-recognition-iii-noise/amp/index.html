
<head>
    <meta charset="utf-8">

    <title>Demystifying Face Recognition III: Noise</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../../../../favicon.png">

    <link rel="shortcut icon" href="../../../../../favicon.png" type="image/png">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="BLCV - Bartosz Ludwiczuk Computer Vision">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Demystifying Face Recognition III: Noise">
    <meta property="og:description" content="How noisy data impact final accuracy?">
    <meta property="og:url" content="http://localhost:2368/2017/11/09/demystifying-face-recognition-iii-noise/">
    <meta property="og:image" content="http://localhost:2368/content/images/2017/11/logo-1.png">
    <meta property="article:published_time" content="2017-11-09T16:15:35.000Z">
    <meta property="article:modified_time" content="2017-11-10T12:23:14.000Z">
    <meta property="article:tag" content="face-recogition">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Demystifying Face Recognition III: Noise">
    <meta name="twitter:description" content="How noisy data impact final accuracy?">
    <meta name="twitter:url" content="http://localhost:2368/2017/11/09/demystifying-face-recognition-iii-noise/">
    <meta name="twitter:image" content="http://localhost:2368/content/images/2017/11/logo-1.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Bartosz Ludwiczuk">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="face-recogition">
    <meta property="og:image:width" content="1402">
    <meta property="og:image:height" content="342">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "BLCV - Bartosz Ludwiczuk Computer Vision",
        "logo": "http://localhost:2368/content/images/2017/11/logo.png"
    },
    "author": {
        "@type": "Person",
        "name": "Bartosz Ludwiczuk",
        "url": "http://localhost:2368/author/bartosz/",
        "sameAs": []
    },
    "headline": "Demystifying Face Recognition III: Noise",
    "url": "http://localhost:2368/2017/11/09/demystifying-face-recognition-iii-noise/",
    "datePublished": "2017-11-09T16:15:35.000Z",
    "dateModified": "2017-11-10T12:23:14.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:2368/content/images/2017/11/logo-1.png",
        "width": 1402,
        "height": 342
    },
    "keywords": "face-recogition",
    "description": "How noisy data impact final accuracy?",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 1.15">
    <link rel="alternate" type="application/rss+xml" title="BLCV - Bartosz Ludwiczuk Computer Vision" href="../../../../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-iframe" src="https://cdn.ampproject.org/v0/amp-iframe-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../../../../">BLCV - Bartosz Ludwiczuk Computer Vision</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Demystifying Face Recognition III: Noise</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../../../../author/bartosz/">Bartosz Ludwiczuk</a></p>
                    <time class="post-date" datetime="2017-11-09">2017-11-09</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="http://localhost:2368/content/images/2017/11/logo-1.png" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <div class="kg-card-markdown"><p>In last years there were introduced many dataset for Face Recognition, named several:</p>
<ul>
<li><a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html">CASIA-WebFace</a>: 0.5M of images</li>
<li><a href="http://vintage.winklerbros.net/facescrub.html">FaceScrub</a>: 0.1M of images</li>
<li><a href="http://www.msceleb.org/celeb1m/dataset">MsCeleb</a>: 10M of images</li>
<li><a href="http://www.umdfaces.io">UMD</a>: 0.38M of images</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face/">VGG</a>: 2.6M of images</li>
</ul>
<p>The main difference between them are the number of images and identities. But they're also different in sth like <code>signal-to-noise ratio</code>, where we mean that ratio between correctly labeled images to all collected images. We do not know such value for any provided database, but some researcher provided the clean-list which provided images which was verified to be correct (by human or face-recognition algorithm). For example <strong>CASIA-WebFace</strong> have 90% of correct images, in the other side <strong>VGG</strong> have only 38% (rest of images have noisy labels). But what does it mean relative to accuracy of Face Recognition model? Does noise really hurt the performance in verification/identification protocol?</p>
<h2 id="noisydatabase">Noisy Database</h2>
<p>First of all, let's make some literature review.<br>
One of the publication which we know about noise in data for Deep Learning is <a href="https://arxiv.org/abs/1511.06789">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</a>. The idea was simple: instead of marking pictures manually use information gathered in Web (metadata and search-engine). This enable to collect many more images than existed in current training dataset of several benchmarks. The results indicate that when training on original images and from web, the final accuracy is always higher (despite the fact, that the data in noisy).</p>
<p>The first public face-recognition database with significant amount of noise was <a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf">VGG</a>. We will describe their method in more details. They collected 5M images from both Google and Bing Image Search. Then using Machine Learning algorithm with Fisher Vector Faces descriptor, half of images was removed. The last stage was manually filtering, which take 10 days (really hard work), ending with ~1M of images. Additional the images before manually filtering were provided, what end up with 2.6M. In experiments, Oxford team was using both dataset (named full and curated). And the bigger one leads to better results at all of their experiments, although it contain more noisy images than right one. Before jumping to any conclusion, let's take a closer look into manually filtering process by reading the paper:</p>
<blockquote>
<p>The aim of this final stage is to increase the purity (precision) of the data using human annotations. However, in order to make the annotation task less burdensome and hence avoid high annotation costs, annotators are aided by using automatic ranking once more. This time, however, a multi-way CNN is trained to discriminate between the 2,622 face identities using the AlexNet architecture; then the softmax scores are used to rank images within each identity set by decreasing likelihood of being an inlier. In order to accelerate the work of the annotators, the ranked images of each identity are displayed in blocks of 200 and annotators are asked to validate blocks as a whole. In particular, a block is declared good if approximate purity is greater than 95%.</p>
</blockquote>
<p>So filtering was done using block of images, which were accepted or refused by looking into purity of it. It means that if there is even some noise in block (more than 5%), then block is removed. To sum up, in 1.6M of noisy images there could be even ~1.4 of good labels. The similar situation is at other dataset like <strong>CASIA</strong> or <strong>MsCeleb</strong>: we are not sure that noise is really a noise, because most of time manually annotators does not investigate each of image independently. But from both of them researchers provided list of clean images by removing the noise manually (CASIA) or in automatic way (MsCeleb).</p>
<h2 id="experimentsoncasiawebface">Experiments on CASIA-WebFace</h2>
<p>The first test will be based on <strong>UMD</strong> database (as very clean database) and different version of <strong>CASIA-WebFace</strong> database:</p>
<ul>
<li>full (0.5M of images)</li>
<li>clean-v1 (0.47M, taken from <a href="https://github.com/happynear/FaceVerification">Happy-Near page</a>)</li>
<li>clean-v2 (0.445M, taken from project <a href="http://zhengyingbin.cc/ActiveAnnotationLearning">Face Recognition via Active Annotation and Learning</a>)</li>
</ul>
<p>Idea behind using this database is testing algorithm on very clean database with smaller size (UMD) versus bigger and noisier database with some efforts of cleaning it.</p>
<p>For training the network, we will be using the same setting like for baseline. Here are results.</p>
<p align="center">
<amp-iframe width="620" height="300" frameborder="0" src="https://plot.ly/~melgor89/108.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
</p>
<p>There is no clean conclusion from this experiments. First of all, we can see that delivered clean-list for CASIA-WebFace really have lower noise, because network trained using them have lower loss and higher validation accuracy. But this is the main difference, all network achieve similar score at LFW and BLUFR benchmarks. The highest score achieve the cleanest one, <strong>clean-v2</strong>, but the difference is only ~1%. Maybe if dataset would contain more images, the bigger change would be noticed. <strong>UMD</strong> database despite of being very clean, achieve the lowest score.</p>
<h2 id="experimentsonvggdatabase">Experiments on VGG-Database</h2>
<p>As the first experiments does not make any clean conclusion, let's use <strong>VGG</strong> database. As the level of noisy labels in VGG can be much bigger, here will be conducted  experiments using different ratio of noise (exactly images labeled as noise to all images in training set). We hope that it will show how different amount of noise impact the final accuracy (remembering that the noise in case of VGG maybe not a real noise). For coherence of results, in each experiment same validation dataset is used.</p>
<p align="center">
<amp-iframe width="750" height="400" frameborder="0" src="https://plot.ly/~melgor89/112.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
</p>
<p>Unfortunately, the results are again not conclusive. In contrast to earlier results, here more noise make result better. We think that it is because of noisy labels, which have a lot of true labels.</p>
<h2 id="experimentsusingrealnoise">Experiments using Real Noise</h2>
<p>In such situation, we decided to carry out last experiment. Using the cleanest version of CASIA-WebFace we will be injecting real noise to data. The noise will be formed from images taken from VGG database with random labels. As we would like to have a small amount of noise in base training data, we will use <strong>clean-v2</strong> list.</p>
<p align="center">
<amp-iframe width="750" height="400" frameborder="0" src="https://plot.ly/~melgor89/114.embed" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
</p>
<p>Finally, this experiments provide us clear conclusion. The real noise, even in small amout, hurt the final accuracy. Although the model with 10% of noise achieve better score at LFW, the BLUFR protocol show the real difference. So we can conclude that is nice to have less than 10% of noise. Also strange situation happened with noise ratio of 50%: the network diverge, so we should lower the learning-rate.</p>
<p>In the other hard, it is really intriguing that having the 80% of noisy labels still network can learn anything useful. The similar result was achieved by researcher in paper <a href="https://arxiv.org/abs/1705.10694">Deep Learning is Robust to Massive Label Noise</a>, even that the experiments have 99% of noise. The researcher have very novel conclusions:</p>
<ul>
<li>Deep neural networks are able to learn from data that has been diluted by an arbitrary amount of noise.</li>
<li>A sufficiently large training set is more important than a lower level of noise.</li>
<li>Choosing good hyperparameters can allow conventional neural networks to operate in the regime of very high label noise.</li>
</ul>
<p>For sure Deep Learning is amazing in handling the noisy data, but we cannot agree with second conclusion, already owning ~0.5M of data example. Having 0.4M of images with clean labels is better than any database with larger amount of images but with noise. Other pros of having clean and small database is faster convergence (as there is less training examples). In the other hand, when we are collecting our own dataset it is inevitable to have noisy labels. Our experiments show that data should not have more than 10% of real noise.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Look like noisy labels impact the final accuracy of model, so it is not a good idea to artificially make dataset bigger by injecting of noise (like a data augmentation method). Also, the cleaner dataset with enough training examples the better. It is contradict to conclusion from paper <a href="https://arxiv.org/abs/1511.06789">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</a>. It is because of different initial assumptions as their dataset have sizes: 6k, 10k, 20k, 50k. So each of them is much smaller that we used. In other hand, if our noisy dataset have sth like &gt;90% of good labels, it is recommended to use it at training as well.<br>
In such situation we need to agree with conclusion from <a href="https://arxiv.org/abs/1705.10694">Deep Learning is Robust to Massive Label Noise</a>, with slight modification:</p>
<blockquote>
<p>A sufficiently large training set is more important than a lower level of noise only if your original dataset have not enough data (which number depend on task, ex. Face-Recognition is ~0.4M)</p>
</blockquote>
<p>Final conclusion:</p>
<ol>
<li>Cleaning the dataset can lead to better accuracy, but gain will be minimal if there were less than 10% of noise.</li>
<li>If we have database which have only 5%-10% of noisy labels, it is advised to use it for training.</li>
<li>Adding images with real noise is not beneficial.</li>
</ol>
<p>Ok, so test another aspect of preparing the dataset for Face-Recognition. The next question which will we ask about pipeline is:<br>
<strong>How the input image should be aligned before feeding it to network?</strong></p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://arxiv.org/abs/1411.7923">Learning Face Representation from Scratch”</a></li>
<li><a href="http://vintage.winklerbros.net/Publications/icip2014a.pdf">A data-driven approach to cleaning large face datasets.</a></li>
<li><a href="https://arxiv.org/abs/1607.08221">MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition</a></li>
<li><a href="https://arxiv.org/abs/1611.01484v2">UMDFaces: An Annotated Face Dataset for Training Deep Networks</a></li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf">VGG-Deep Face Recognition</a></li>
<li><a href="https://arxiv.org/abs/1511.06789">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</a></li>
<li><a href="https://arxiv.org/abs/1705.10694">Deep Learning is Robust to Massive Label Noise</a></li>
<li><a href="http://zhengyingbin.cc/ActiveAnnotationLearning">Face Recognition via Active Annotation and Learning</a></li>
<li><a href="https://github.com/happynear/FaceVerification">Happy-Near page</a></li>
</ul>
</div>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../../../../">BLCV - Bartosz Ludwiczuk Computer Vision</a> © 2017</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
